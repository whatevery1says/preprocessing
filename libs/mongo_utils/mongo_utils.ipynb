{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB utilities\n",
    "\n",
    "Development notebook for two sets of utilities:\n",
    "\n",
    "1. mongo_utils.py: general purpose\n",
    "2. we1s_mongo_utils: project-specific\n",
    "\n",
    "Below are examples of reporting queries for inspecting the WE1S mongodb collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mongo_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mongo_utils.py\n",
    "General purpose tools for working with pymongo and mongodb.\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from pymongo import MongoClient \n",
    "from pymongo.errors import DuplicateKeyError, InvalidDocument\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, compact=False)\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0, '/home/jovyan/utils/preprocessing/')\n",
    "# from libs.fuzzyhasher.fuzzyhasher import FuzzyHasher\n",
    "# from libs.zipeditor.zipeditor import ZipEditor, zip_scanner, zip_scanner_excludedirs, ZipProcessor\n",
    "# from we1s_utils.ziputils import BatchJSONUploader\n",
    "\n",
    "# filter = {\"name\": {\"$regex\": r\"^(?!system\\.)\"}}\n",
    "\n",
    "# def _tabulate_row(*row):\n",
    "#     return ''.join(str(word).ljust(12) for word in row)\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "##  mongodb doc generators  ##\n",
    "##############################\n",
    "## these two (client_colls and db_colls)\n",
    "## and their dependents might be refactored\n",
    "## -- as it turns out, the database and client\n",
    "## are retrievable from the collection object,\n",
    "## so interfaces to collection level function\n",
    "## could be simplified.\n",
    "##\n",
    "## client = MongoClient('mongodb://mongo/')\n",
    "## db = client['we1s']\n",
    "## coll = db['deletes_humanities']\n",
    "##\n",
    "## print(coll)\n",
    "## print(coll.name)\n",
    "## print(coll.database)\n",
    "## print(coll.database.name)\n",
    "## print(coll.database.client)\n",
    "##\n",
    "## OUTPUT:\n",
    "## Collection(Database(MongoClient(host=['mongo:27017'], document_class=dict, tz_aware=False, connect=True), 'we1s'), 'deletes_humanities')\n",
    "## deletes_humanities\n",
    "## Database(MongoClient(host=['mongo:27017'], document_class=dict, tz_aware=False, connect=True), 'we1s')\n",
    "## we1s\n",
    "## MongoClient(host=['mongo:27017'], document_class=dict, tz_aware=False, connect=True)\n",
    "## Database(MongoClient(host=['mongo:27017'], document_class=dict, tz_aware=False, connect=True), 'name')\n",
    "\n",
    "def client_colls(client, filter=None):\n",
    "    \"\"\"All collections across all databases--generator.\"\"\"\n",
    "    d = dict((db, [collection for collection in client[db].list_collection_names(filter=filter)])\n",
    "             for db in client.list_database_names())\n",
    "    for db in d:\n",
    "        for coll in d[db]:\n",
    "            yield client, db, coll\n",
    "            # yield coll\n",
    "        \n",
    "def db_colls(client, db, filter=None):\n",
    "    \"\"\"All collections in a database--generator.\"\"\"\n",
    "    db_coll_list = [collection for collection in client[db].list_collection_names(filter=filter)]\n",
    "    for coll in db_coll_list:\n",
    "        yield client, db, coll\n",
    "        # yield coll\n",
    "\n",
    "\n",
    "\n",
    "def mdbkey_decode(key):\n",
    "    \"\"\"Decode encoded mongodb key to recover original string.\"\"\"\n",
    "    return key.replace(\"\\\\u002e\", \".\").replace(\"\\\\u0024\", \"\\$\").replace(\"\\\\\\\\\", \"\\\\\")\n",
    "\n",
    "def mdbkey_encode(key):\n",
    "    \"\"\"Encode key field with . or $ to be valid mongodb key.\"\"\"\n",
    "    return key.replace(\"\\\\\", \"\\\\\\\\\").replace(\".\", \"\\\\u002e\").replace(\"\\$\", \"\\\\u0024\")\n",
    "\n",
    "def mdbkey_strip(key):\n",
    "    \"\"\"Strip . or $ to enforce valid mongodb key.\"\"\"\n",
    "    return key.replace(\".\", \"\").replace(\"$\", \"\")\n",
    "\n",
    "def print_doc(doc, pop_list=None, trim_dict=None):\n",
    "    \"\"\"Cleans up verbose mongodb JSON documents for preview\n",
    "    using a list of top-level fields to pop or trim.\n",
    "    \"\"\"\n",
    "    if pop_list:\n",
    "        for key in pop_list:\n",
    "            if key in doc:\n",
    "                doc.pop(key)\n",
    "    if trim_dict:\n",
    "        for key, value in trim_dict.items():\n",
    "            if key in doc:\n",
    "                doc[key] = doc[key][0:value]\n",
    "    pp.pprint(doc)\n",
    "\n",
    "\n",
    "def print_query(query, style='pp'):\n",
    "    \"\"\"Convenience method for displaying pymongo\n",
    "    queries in readable formats. Three styles:\n",
    "    PrettyPrinter, json.dumps(), or print().\n",
    "    \n",
    "    Examples:\n",
    "        >>> query = {'$or': [{'name': {'$regex': '.*liberal.*'}}, {'name': {'$regex': '.*humanities.*'}}]}\n",
    "        >>> print_query(query)\n",
    "        {   '$or': [   {'name': {'$regex': '.*liberal.*'}},\n",
    "                       {'name': {'$regex': '.*humanities.*'}}]}\n",
    "\n",
    "        >>> print_query(query, style=None)\n",
    "        {'$or': [{'name': {'$regex': '.*liberal.*'}}, {'name': {'$regex': '.*humanities.*'}}]}\n",
    "\n",
    "        >>> print_query(query, style='json')\n",
    "        {\n",
    "            \"$or\": [\n",
    "                {\n",
    "                    \"name\": {\n",
    "                        \"$regex\": \".*liberal.*\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": {\n",
    "                        \"$regex\": \".*humanities.*\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    if style=='pp':\n",
    "        # pprint adds linebreaks and indents, yet stays compact\n",
    "        pp.pprint(query)\n",
    "    elif style=='json':\n",
    "        # json can also be used to print nested dict/lists\n",
    "        # in a more articulated indented outline form\n",
    "        print(json.dumps(query, sort_keys=True, indent=4))\n",
    "    else:\n",
    "        # pymongo queries are python dicts of dicts (or lists of dicts)\n",
    "        # however, these nested lines may be hard to read        \n",
    "        print(query)\n",
    "\n",
    "def _test_print_query():\n",
    "    \"\"\"Test three output formats\"\"\"\n",
    "    print('Display a mongodb query in three different print formats.')\n",
    "    query = {'$or': [{'name': {'$regex': '.*liberal.*'}}, {'name': {'$regex': '.*humanities.*'}}]}\n",
    "    print('----------')\n",
    "    print_query(query, style='None')\n",
    "    print('----------')\n",
    "    print_query(query)\n",
    "    print('----------')\n",
    "    print_query(query, style='json')\n",
    "\n",
    "def print_table(rows):\n",
    "    for row in rows:\n",
    "        print(''.join(str(word).ljust(12) for word in row))\n",
    "\n",
    "def print_SON(son):\n",
    "    if son:\n",
    "        print(index.to_dict())\n",
    "\n",
    "def report_aggregate(coll_list, pipeline):\n",
    "    \"\"\"\n",
    "    pipeline_pub = [\n",
    "        {'$group' : {'_id' : '$pub', 'count' : {'$sum' : 1}}},\n",
    "        { '$sort' : {'count' : -1} }\n",
    "    ]\n",
    "    pipeline_term = [\n",
    "        {'$group' : {'_id' : '$term', 'count' : {'$sum' : 1}}},\n",
    "        { '$sort' : {'count' : -1} }\n",
    "    ]\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    for client, db, coll in coll_list:\n",
    "        result = client[db].command('aggregate', coll, pipeline=pipeline, explain=False)\n",
    "        for row in result:\n",
    "            if row:\n",
    "                report.append(json.dumps(row, sort_keys=True, indent=2))\n",
    "    return report\n",
    "\n",
    "def report_collstats(coll_list, key='count', header=True):\n",
    "    \"\"\"Return a stat (e.g. document count) for each mongodb\n",
    "    collection list.\n",
    "    May be passed a generator such as client_colls or db_colls.\n",
    "    Pretty-print with print_table().\n",
    "\n",
    "    Examples:\n",
    "        >>> report_collstats((client, 'we1s', 'reddit'))\n",
    "        count       db          coll      \n",
    "        12345       we1s        reddit    \n",
    "\n",
    "        >>> report_collstats(db_colls(client, 'we1s'), key='avgObjSize')\n",
    "        avgObjSize  db          coll      \n",
    "        19788       we1s        reddit    \n",
    "        105391      we1s        humanities_keywords_no_exact\n",
    "\n",
    "    Args:\n",
    "        coll_list (tuple): (client, db_name, coll_name)\n",
    "        key (str): the stat to report.\n",
    "        header (bool): Include column headers in output\n",
    "\n",
    "    Returns:\n",
    "        A list of the key value for each db.collection:\n",
    "        [(key, db, coll),\n",
    "         (key, db, coll),\n",
    "         (key, db, coll)]\n",
    "    \"\"\"\n",
    "\n",
    "    report = []\n",
    "    for client, db, coll in coll_list:\n",
    "        if key:\n",
    "            value = client[db].command(\"collstats\", coll)[key]\n",
    "        else:\n",
    "            value = client[db].command(\"collstats\", coll)\n",
    "        report.append((value, db, coll))\n",
    "    if report:\n",
    "        if header:\n",
    "            report.insert(0, (key, 'db', 'coll'))\n",
    "        return report\n",
    "\n",
    "def report_indexes(coll_list):\n",
    "    \"\"\"Return all indexes for each mongodb collection in a list.\n",
    "    May be passed a generator such as client_colls or db_colls.\n",
    "    Pretty-print with print_SON().\n",
    "    \n",
    "    Args:\n",
    "        coll_list (tuple): (client, db_name, coll_name)\n",
    "\n",
    "    Returns:\n",
    "        A list of indexes (SON objects).\n",
    "        [SON, SON, SON]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for client, db, coll in coll_list:\n",
    "        for index in client[db][coll].list_indexes():\n",
    "            if index:\n",
    "                result.append(index)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MongoCounter:\n",
    "    \"\"\"For a mongo server or collection, build a list of FieldCounters\"\"\"\n",
    "\n",
    "    def __init__(self, client=None, progress=None):\n",
    "        self.results = []\n",
    "        self.client = client\n",
    "        self.progress = progress\n",
    "\n",
    "    def _client_dbs_command(self, client, command='collstats'):\n",
    "        \"\"\"Loop over all dbs and all collections,\n",
    "        returning command result for each collection.\n",
    "        \"\"\"\n",
    "        d = dict((db, [collection for collection in client[db].list_collection_names()])\n",
    "                     for db in client.list_database_names())\n",
    "        for db in d:\n",
    "            for coll in d[db]:\n",
    "                # print(db, coll)\n",
    "                yield client[db].command(command, coll), db, coll\n",
    "\n",
    "    def _tabulate(self, *row):\n",
    "        return ''.join(str(word).ljust(10) for word in row)\n",
    "\n",
    "#     # DEPRECATED\n",
    "#     def count_docs(self, client=None):\n",
    "#         \"\"\"Display document counts for all collections in all dbs.\n",
    "#         e.g.\n",
    "#         79 local startup_log\n",
    "#         752243 we1s reddit\n",
    "#         418302 we1s humanities_keywords\n",
    "#         \"\"\"\n",
    "#         if not client: client = self.client\n",
    "#         counts = self._client_dbs_command(client, command='collstats')\n",
    "#         for collstats, db, coll in counts:\n",
    "#             yield collstats['count'], db, coll\n",
    "\n",
    "#     def count_docs_report(self, client=None):\n",
    "#         if not client: client = self.client\n",
    "#         counts = self.count_docs(client)\n",
    "#         for collstats, db, coll in counts:\n",
    "#             print(self._tabulate(collstats, db + '.' + coll))\n",
    "            \n",
    "    def count_fields_all(self,  client=None, progress=None, show_complete=True):\n",
    "        if not client: client = self.client\n",
    "        for db in client.list_database_names():\n",
    "            self.count_fields_db(db, client, progress, show_complete)\n",
    "        \n",
    "    def count_fields_db(self, db, client=None, progress=None, show_complete=True):\n",
    "        if not client: client = self.client\n",
    "        db_colls = [collection for collection in client[db].list_collection_names()]\n",
    "        for coll in db_colls:\n",
    "            self.count_fields_collection(coll, db, client, progress, show_complete)\n",
    "    \n",
    "    def count_fields_collection(self, coll, db, client=None, progress=None, show_complete=True):\n",
    "        if not client: client = self.client\n",
    "        if progress==None and self.progress:\n",
    "            progress=self.progress\n",
    "        coll_count = client[db].command(\"collstats\", coll)['count']\n",
    "        fcounter = FieldCounter(name=db+'.'+coll + '(' + str(coll_count) + ' docs)', show_complete=show_complete)\n",
    "        cursor = client[db][coll].find({})\n",
    "        print(fcounter)\n",
    "        for doc in cursor:\n",
    "            if progress and fcounter.total !=0 and fcounter.total % progress == 0:\n",
    "                print(fcounter)\n",
    "            fcounter.count_fields(doc)\n",
    "        # print('\\n\\n', '[FINAL]')\n",
    "        print(fcounter.report())\n",
    "        self.results.append(fcounter)\n",
    "\n",
    "#     # DEPRECATED\n",
    "#     def list_indexes(self, client=None):\n",
    "#         \"\"\"Loop over all dbs and all collections,\n",
    "#         returning command result for each collection.\n",
    "#         \"\"\"\n",
    "#         if not client: client = self.client\n",
    "#         d = dict((db, [collection for collection in client[db].list_collection_names()])\n",
    "#                      for db in client.list_database_names())\n",
    "#         for db in d:\n",
    "#             for coll in d[db]:\n",
    "#                 for index in client[db][coll].list_indexes():\n",
    "#                     print(index, db, coll)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join([result for result in self.results])\n",
    "\n",
    "\n",
    "class FieldCounter:\n",
    "    \"\"\"Build counters of dictionary fields.\n",
    "    Useful for surveying mongodb collections for presence/absence of keys across documents.\n",
    "    Keeps two fields counters:\n",
    "    \n",
    "    -  counter: present fields with value\n",
    "    -  counter_empties: present fields that are false (None, 0, '', false, etc.)\n",
    "    ...and:\n",
    "    -  total: number of documents counted\n",
    "    \n",
    "    Supports includes and excludes -- lists of fields to include or ignore.\n",
    "    If includes are defined then only includes will be counted -- unless they\n",
    "    are subsequently filtered by the excludes list.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='', show_complete=True, includes=None, excludes=None):\n",
    "        self.name = name\n",
    "        self.show_complete = show_complete\n",
    "        self.includes = includes\n",
    "        self.excludes = excludes\n",
    "        self.clear()\n",
    "\n",
    "    def _tabulate(self, *row):\n",
    "        return ''.join(str(word).ljust(10) for word in row)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.counter = Counter()\n",
    "        self.counter_empties = Counter()\n",
    "        self.total = 0\n",
    "\n",
    "    def count_collection(self, docs, includes=None, excludes=None):\n",
    "        if not includes: includes = self.includes\n",
    "        if not excludes: excludes = self.excludes\n",
    "        for doc in docs:\n",
    "            yield self.count_fields(doc, includes, excludes)\n",
    "\n",
    "    def count_fields(self, doc, includes=None, excludes=None):\n",
    "        \"\"\"\"\"\"\n",
    "        if not includes: includes = self.includes\n",
    "        if not excludes: excludes = self.excludes\n",
    "        for key, value in doc.items():\n",
    "            if (not includes or key in includes) and (not excludes or key not in excludes):\n",
    "                if value:\n",
    "                    self.counter[key] += 1\n",
    "                else:\n",
    "                    self.counter_empties[key] += 1\n",
    "        self.total+=1\n",
    "        return doc, self.total\n",
    "\n",
    "    def report(self, header=False):\n",
    "        result = ''\n",
    "        if header:\n",
    "            result = self.__str__()\n",
    "        if self.total > 0:\n",
    "            colnames = self._tabulate('found', 'empties', 'missing', 'key')\n",
    "            emptycount = 0\n",
    "            entries = ''\n",
    "            for key, value in sorted(self.counter.items()):\n",
    "                if value != self.total:\n",
    "                    emptycount = self.counter_empties.get(key, 0)\n",
    "                if self.show_complete or value != self.total:\n",
    "                    entries += self._tabulate(value, emptycount, self.total-value, key) + '\\n'\n",
    "            if entries:\n",
    "                result = '\\n'.join([result, colnames, entries])\n",
    "                result += self._tabulate(str(self.total), 'counted') + '\\n'\n",
    "            else:\n",
    "                result += '[no fields empty/missing]\\n'\n",
    "        else:\n",
    "            result += '[no docs]\\n'\n",
    "        return result\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRIP FIELD WHITESPACE\n",
    "\n",
    "\n",
    "def mongo_coll_field_update(coll, field, func):\n",
    "    \"\"\"Given a collection, updates each doc with the field,\n",
    "    changing it to using the lambda function.\"\"\"\n",
    "    hits = 0\n",
    "    for doc in coll.find({field: {'$exists': True, '$ne': []}}, {field:1}):\n",
    "        if mongo_field_update(coll, doc, field, func):\n",
    "            hits += 1\n",
    "    return hits\n",
    "\n",
    "def mongo_field_update(coll, doc, field, func):\n",
    "    \"\"\"Given a doc, changes a field and updates it in mongodb.\"\"\"\n",
    "    if field_update(doc, field, func):\n",
    "        # print('*', end='')\n",
    "        coll.update_one({'_id': doc['_id']},\n",
    "                        {'$set': {field: doc[field]} },\n",
    "                        upsert=False)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def field_update(doc, field, func):\n",
    "    \"\"\"Takes a doc, field name, and lambda function for the field.\n",
    "    Changes the field in place and returns True if updated.\n",
    "    \"\"\"\n",
    "    if field in doc and doc[field]:\n",
    "        val = func(doc[field])\n",
    "        if doc[field] != val:\n",
    "            doc[field] = val\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def func_strip(x):\n",
    "    \"\"\"Many fields have leading and trailing spaces,\n",
    "    leading to e.g. four different \"The New York Times\".\n",
    "    for a collection, iterate over all pub fields and,\n",
    "    if there are leading or trailing spaces, strip.\n",
    "    this should really be part of a pre-import validator.\n",
    "    \"\"\"\n",
    "    return x.strip()\n",
    "    # this could also be passed without a function as:\n",
    "    #   lambda x: x.strip()\n",
    "\n",
    "    \n",
    "#\n",
    "#  DEPRECATED \n",
    "#\n",
    "#  def strip_field(db, collection, field):\n",
    "#     \"\"\"Many fields have leading and trailing spaces,\n",
    "#     leading to e.g. four different \"The New York Times\".\n",
    "#     for a collection, iterate over all pub fields and,\n",
    "#     if there are leading or trailing spaces, strip.\n",
    "#     this should really be part of a pre-import validator.\n",
    "#     \"\"\"\n",
    "#     test_collect = client['we1s']['deletes_humanities']\n",
    "#     hits = 0\n",
    "#     fixed = 0\n",
    "#     for doc in test_collect.find({'pub': {'$exists': True, '$ne': []}}, {'pub':1}):\n",
    "#         hits += 1\n",
    "#         if 'pub' in doc and doc['pub']:\n",
    "#             if doc['pub'] != doc['pub'].strip():\n",
    "#                 fixed += 1\n",
    "#                 test_collect.update_one({'_id': doc['_id']},\n",
    "#                                     {'$set': {'pub': doc['pub'].strip()} },\n",
    "#                                     upsert=False)\n",
    "#     print('hits:  ', hits)\n",
    "#     print('fixed: ', fixed)\n",
    "# \n",
    "# strip_field(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Strip whitespace from field in all articles in a collection.')\n",
    "coll = client['we1s']['deletes_humanities']\n",
    "hits = mongo_coll_field_update(coll, 'term', lambda x: x.strip())\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we1s_mongo_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"we1s_mongo_utils.py\n",
    "WE1S-specific importing and updating tools for pymongo and mongodb.\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pymongo\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, compact=False)\n",
    "\n",
    "def doc_preview(doc, pop_list=None, trim_dict=None, trim_mark='...', object=False, width=None):\n",
    "    preview = copy.deepcopy(doc)\n",
    "    if pop_list:\n",
    "        for key in pop_list:\n",
    "            if key in preview:\n",
    "                preview.pop(key)\n",
    "    if trim_dict:\n",
    "        for key, value in trim_dict.items():\n",
    "            if key in preview:\n",
    "                if len(key) <= value and len(key) > len(trim_mark):\n",
    "                    preview[key] = ''.join([preview[key][0:value-len(trim_mark)], trim_mark])\n",
    "    if object:\n",
    "        return preview\n",
    "    if width: \n",
    "        return pprint.pformat(preview, width=width)\n",
    "    return pp.pformat(preview)\n",
    "\n",
    "class SourcesProcessor:\n",
    "    \"\"\"Documents the pipeline for populating mongodb source docs\n",
    "    and aliases from a csv spreadsheet.\n",
    "    \n",
    "    Use:\n",
    "        csv_to_mongo(client, filepath)\n",
    "        \n",
    "    Internal class methods are stages in the pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 client=MongoClient('mongodb://mongo/'),\n",
    "                 file_path='../sources_master.csv',\n",
    "                 source_path=['Sources','Sources'],\n",
    "                 aliases_path=['Sources','config','source_aliases']):\n",
    "        self.client = client\n",
    "        self.file_path = file_path\n",
    "        self.source_path = source_path\n",
    "        self.aliases_path = aliases_path\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.aliases = {}\n",
    "        self.source_docs = {}\n",
    "    \n",
    "    def get_csv_put_mongo(self):\n",
    "        self.get_csv()\n",
    "        self.put_mongo_aliases()\n",
    "        self.put_mongo_source_docs()\n",
    "\n",
    "    def get_csv(self):\n",
    "        \"\"\"Given a DictReader, returns sources docs and an aliases lookup as data structures.\n",
    "        \n",
    "        The data format is:\n",
    "        title,name,canonical_title,tags1,tags2,tags3,tags4,tags5,tags6,country,language\n",
    "        \n",
    "        mockup of a Source manifest:\n",
    "        {\n",
    "            \"name\": \"advance-titan-university-of-wisconsin-oshkosh\",\n",
    "            \"metapath\": \"Sources\",\n",
    "            \"namespace\": \"we1s2.0\",\n",
    "            \"title\": \"Advance-Titan: University of Wisconsin - Oshkosh\",\n",
    "            \"country\": \"US\",\n",
    "            \"language\": \"en\",\n",
    "            \"tags\": [\"region/US/Midwest\", \"education/funding/US public college\"],\n",
    "            \"collection_identifiers\": [\"Advance\", \"Advance-Titan\", \"Advance-Titan: University of Wisconsin - Oshkosh\"]\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        self.aliases = {} # alias-to-name lookup entries\n",
    "        self.source_docs = {}  # sources docs\n",
    "        self.csv_parse_log = []\n",
    "        name_prev = ''\n",
    "        csvfile = open(self.file_path, 'r')\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            \n",
    "            # build source_name_aliases\n",
    "            akey = mdbkey_encode(row['alias'])\n",
    "            dkey = row['name'].strip()\n",
    "            if akey and akey not in self.aliases:\n",
    "                self.aliases[akey] = dkey\n",
    "            else:\n",
    "                self.csv_parse_log.append(\"duplicate key '{0}' found: '{1}'\".format(akey, row['name'].strip()))\n",
    "                # raise ValueError(\"duplicate key '{0}' found\".format(akey))\n",
    "    \n",
    "            # build source_docs\n",
    "            if dkey in self.source_docs:\n",
    "                # if name is a repeat, add an alias and continue\n",
    "                self.source_docs[dkey]['aliases'].append(row['alias'].strip())\n",
    "            else:\n",
    "                # if source name is new, create\n",
    "                # this is an explicit add -- new named columns in the sheet won't be imported\n",
    "                self.source_docs[dkey] = {}\n",
    "                self.source_docs[dkey]['_id'] = dkey\n",
    "                self.source_docs[dkey]['name'] = dkey\n",
    "                self.source_docs[dkey]['country'] = row['country']\n",
    "                self.source_docs[dkey]['language'] = row['language']\n",
    "                self.source_docs[dkey]['metapath'] = 'Sources'\n",
    "                self.source_docs[dkey]['namespace'] = 'we1s2.0'\n",
    "                self.source_docs[dkey]['title'] = row['canonical_title'].strip()\n",
    "                tags = [row['tags1'],row['tags2'],row['tags3'],row['tags4'],row['tags5'],row['tags6']]\n",
    "                self.source_docs[dkey]['tags'] = [x for x in tags if x]\n",
    "                self.source_docs[dkey]['aliases'] = [row['alias'].strip()]\n",
    "    \n",
    "    def get_mongo_aliases(self):\n",
    "        \"\"\"get aliases from db\"\"\"\n",
    "        ap = self.aliases_path\n",
    "        self.aliases = self.client[ap[0]][ap[1]].find_one({'_id' : ap[2]})['aliases']\n",
    "\n",
    "    def get_mongo_source_docs(self):\n",
    "        sp = self.source_path\n",
    "        results = self.client[sp[0]][sp[1]].find({})\n",
    "        self.source_docs = {}\n",
    "        for result in results:\n",
    "            self.source_docs[result['_id']] = result        \n",
    "\n",
    "    def put_mongo_aliases(self):\n",
    "        \"\"\"insert aliases into db as doc\"\"\"\n",
    "        doc = {}\n",
    "        doc['_id'] = self.aliases_path[2]\n",
    "        doc['aliases'] = self.aliases\n",
    "        ap = self.aliases_path\n",
    "        self.client[ap[0]][ap[1]].replace_one({'_id' : ap[2]}, doc, upsert=True)\n",
    "\n",
    "    def put_mongo_source_docs(self):\n",
    "        \"\"\"insert source docs into db\"\"\"\n",
    "        sp = self.source_path\n",
    "        for key, source_doc in self.source_docs.items():\n",
    "            client[sp[0]][sp[1]].replace_one({'_id':source_doc['_id']}, source_doc, upsert=True)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(client={self.client}, file_path={self.file_path}, source_path={self.source_path}, aliases_path={self.aliases_path})'\n",
    "\n",
    "\n",
    "class ArticleProcessor:\n",
    "    \"\"\"Dynamic rewriting of article data fields, partricularly\n",
    "    the source field and api fields.\n",
    "    Relies on data from source aliases -- uploaded / retrieved with SourcesProcessor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sources_processor):\n",
    "        self.sp = sources_processor\n",
    "        self.sp.get_mongo_aliases()\n",
    "        self.aliases = sp.aliases\n",
    "        self.source_docs = sp.source_docs\n",
    "\n",
    "    def display_ipython_table(self, data):\n",
    "        from IPython.display import display, HTML\n",
    "        # css_str = '<style>body{background-color:#000000}; table{width:600px !important}; td{width:200 !important};</style>'\n",
    "        css_str = '<style>td{border: 1px solid black} td{text-align:left; vertical-align:top}</style>'\n",
    "        display(HTML(\n",
    "            css_str + '<table><tr style>{}</tr></table>'.format(\n",
    "                '</tr><tr>'.join(\n",
    "                '<td><pre>{}</pre></td>'.format('</pre></td><td><pre>'.join(str(_) for _ in row)) for row in data)\n",
    "            )), metadata=dict(isolated=True))\n",
    "\n",
    "    def json_add_api_fields(self, json_data, name_hint):\n",
    "        \"\"\"Set json api... fields from name hint and optional\n",
    "        database field. Hint may be generated by\n",
    "        json_add_api_fields_guess checking the name field.\n",
    "        \"\"\"\n",
    "        translations = {\n",
    "            'LexisNexis':['we1s-collector', 'LexisNexis'],\n",
    "            'LexisNexis UniversityWire':['we1s-collector', 'LexisNexis', 'UniversityWire'],\n",
    "            'chomp':['chomp','chomp'],\n",
    "            # 'chomp':['chomp','google.com'],\n",
    "            # 'chomp':['chomp','wordpress.com'],\n",
    "            'ProQuest':['ProQuest Global Newsstream','ProQuest'],\n",
    "            'Global Newsstream':['ProQuest Global Newsstream','ProQuest'],\n",
    "            'Global Newsstrea m':['ProQuest Global Newsstream','ProQuest'],\n",
    "            'Globa l Newsstream':['ProQuest Global Newsstream','ProQuest'],\n",
    "            'Global Newss tream':['ProQuest Global Newsstream','ProQuest'],\n",
    "            'Ethnic NewsWatch':['ProQuest Global Newsstream', 'ProQuest','Ethnic NewsWatch'],\n",
    "            'Ethnic NewsWatc h':['ProQuest Global Newsstream','ProQuest','Ethnic NewsWatch'],\n",
    "            'Ethnic N ewsWatch':['ProQuest Global Newsstream','ProQuest','Ethnic NewsWatch'],\n",
    "            'Ethnic NewsWatch; GenderWatch':['ProQuest Global Newsstream','ProQuest','Ethnic NewsWatch; GenderWatch'],\n",
    "            'GenderWatch':['ProQuest Global Newsstream','ProQuest','GenderWatch'],\n",
    "            'reddit':['reddit','reddit'],\n",
    "            'Twitter':['Twitter','Twitter']\n",
    "        }\n",
    "        # manual dictionary\n",
    "        if 'database' in json_data:\n",
    "            # lookups and \n",
    "            labels = translations[json_data['database']]\n",
    "        else:\n",
    "            # fall back to name label\n",
    "            labels = translations[name_hint]\n",
    "    \n",
    "        # set the fields\n",
    "        if 'api_software' not in json_data:\n",
    "            json_data['api_software'] = labels[0]\n",
    "        if 'api_data_provider' not in json_data:\n",
    "            json_data['api_data_provider'] = labels[1]\n",
    "        if(len(labels)>2):\n",
    "            if 'api_data_provider_channel' not in json_data:\n",
    "                json_data['api_data_provider_channel'] = labels[2]\n",
    "    \n",
    "    def json_add_api_fields_guess(self, json_data):\n",
    "        \"\"\"Deduce api fields hint from name, try lookup\n",
    "        and add api_fields to json.\n",
    "        \"\"\"\n",
    "        if 'name' in json_data:\n",
    "            if 'chomp' in json_data['name'].lower() :\n",
    "                self.json_add_api_fields(json_data, 'chomp')\n",
    "            elif 'reddit' in json_data['name'].lower():\n",
    "                self.json_add_api_fields(json_data, 'reddit')\n",
    "            elif 'proquest' in json_data['name'].lower():\n",
    "                self.json_add_api_fields(json_data, 'ProQuest')\n",
    "            elif 'twitter' in json_data['name'].lower():\n",
    "                self.json_add_api_fields(json_data, 'Twitter')\n",
    "            elif 'universitywire' in json_data['name'].lower():\n",
    "                self.json_add_api_fields(json_data, 'LexisNexis UniversityWire')\n",
    "            else:\n",
    "                self.json_add_api_fields(json_data, 'LexisNexis')\n",
    "    \n",
    "    def json_add_source(self, json_data, aliases=None):\n",
    "        \"\"\"use source name aliases dict to lookup and save canonical source name.\n",
    "        \"\"\"\n",
    "        if not aliases: aliases = self.aliases\n",
    "        lookup_name = ''\n",
    "        if 'name' in json_data:\n",
    "            if 'chomp' in json_data['name'].lower() :\n",
    "                # chomp zip:  chomp_vox_humanities_2000-01-01_2020-01-01.zip\n",
    "                # chomp json: chomp_vox_humanities_2000-01-01_2020-01-01_0.json\n",
    "                lookup_name = json_data['name'].split('_')[1]\n",
    "            elif 'reddit' in json_data['name'].lower():\n",
    "                # reddit zip:  reddit-all-the-arts-2006-2018-264.zip\n",
    "                # reddit json: Reddit-The-Arts-All-2006-2018_180.json\n",
    "                lookup_name = 'Reddit'\n",
    "            elif 'proquest' in json_data['name'].lower():\n",
    "                # proquest has no standard format\n",
    "                # proquest  zip: proquest_thewallstreetjournal_humanities_1984_1989.zip\n",
    "                # proquest json: proquest_thewallstreetjournal_humanities_1984_1989_001_.json\n",
    "                if 'thewallstreetjournal' in json_data['name'].lower():\n",
    "                    # proquest-wallstreet exception\n",
    "                    lookup_name = 'thewallstreetjournal'\n",
    "                else:\n",
    "                    lookup_name = json_data['pub']\n",
    "            elif 'universitywire' in json_data['name'].lower():\n",
    "                # LN University Wire\n",
    "                # zip: 172244_universitywire_bodypluralhumanitiesorhleadpluralhumanities_2014-01-01_2014-12-31.zip\n",
    "                # json: 172244_172244_universitywire_bodypluralhumanitiesorhleadpluralhumanities_2014-01-01_2014-12-31_16_0_0.json\n",
    "                lookup_name = json_data['pub']\n",
    "            else:\n",
    "                # LexisNexis default\n",
    "                # zip: 8006_thelatimes_bodypluralhumanitiesorhleadpluralhumanities_2011-01-01_2011-12-31.zip\n",
    "                # json: 8006_8006_thelatimes_bodypluralhumanitiesorhleadpluralhumanities_2011-01-01_2011-12-31_55_0_0.json\n",
    "                lookup_name = json_data['name'].split('_')[2]\n",
    "        source_name = aliases[mdbkey_encode(lookup_name)]\n",
    "        # print('cn:', canonical_name, type(canonical_name))\n",
    "        if 'sources' in json_data:\n",
    "            json_data.pop('sources')\n",
    "        json_data['source'] = source_name\n",
    "\n",
    "    def json_update(self, doc, aliases=None):\n",
    "        \"\"\"Given an in-memory doc, do an in-memory rewrite\n",
    "        based on:\n",
    "            add_source\n",
    "            add_api_fields_guess\n",
    "        This does not add/update the document to a database.\n",
    "        \"\"\"\n",
    "        if not aliases: aliases = self.aliases\n",
    "        # print('rewrite:', doc['_id'])\n",
    "        self.json_add_source(doc, aliases)\n",
    "        self.json_add_api_fields_guess(doc)\n",
    "\n",
    "    def json_update_previews(self, doc, pop_list=None, trim_dict=None, width=None):\n",
    "        \"\"\"Changes the in-memory doc with json_update,\n",
    "        returns two doc_previews with pretty printing: before and after.\n",
    "        \"\"\"\n",
    "        before_prev = doc_preview(doc, pop_list=pop_list, trim_dict=trim_dict, width=width)\n",
    "        self.json_update(doc)\n",
    "        after_prev = doc_preview(doc, pop_list=pop_list, trim_dict=trim_dict, width=width)\n",
    "        return before_prev, after_prev\n",
    "\n",
    "    def mongo_replace_docs(self, docs, collection):\n",
    "        for doc in docs:\n",
    "            self.json_update(doc)\n",
    "            collection.replace_one({'_id': doc['_id']}, doc, upsert=False)\n",
    "    \n",
    "    def mongo_update_docs(self, docs, collection):\n",
    "        for doc in docs:\n",
    "            self.mongo_update_doc(doc, collection)\n",
    "    \n",
    "    def mongo_update_doc(self, doc, collection):\n",
    "        self.json_update(doc)\n",
    "        update_command = {'$set': {'api_data_provider': doc['api_data_provider'].strip(),\n",
    "                                   'api_software': doc['api_software'].strip(),\n",
    "                                   'source': doc['source'].strip()\n",
    "                                  },\n",
    "                          '$unset': { 'sources' : \"\", 'length' : \"\" }}\n",
    "        if 'api_data_provider_channel' in doc:\n",
    "            update_command.setdefault('$set', {})\n",
    "            update_command['$set']['api_data_provider_channel'] = doc['api_data_provider_channel']            \n",
    "        if 'api_data_provider' in doc and 'LexisNexis' in doc['api_data_provider']:\n",
    "            update_command.setdefault('$rename', {})\n",
    "            update_command['$rename']['doc_id'] = 'ln_doc_id'\n",
    "            update_command['$rename']['attachment_id'] = 'ln_attachment_id'\n",
    "        \n",
    "        updated = collection.find_one_and_update({'_id': doc['_id']},\n",
    "                                                 update_command,\n",
    "                                                 return_document=pymongo.ReturnDocument.AFTER,\n",
    "                                                 upsert=False)\n",
    "        return updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## API NOTES\n",
    "\n",
    "# PROQUEST\n",
    "# ADD\n",
    "#   'api_name': 'ProQuest'\n",
    "#   'api_notes' : '2018'\n",
    "# RENAME\n",
    "#   content-hash-ssdeep -> content_hash_ssdeep\n",
    "#   doc_id -> proquest_doc_id\n",
    "#   word_count -> content_word_count\n",
    "# DELETE\n",
    "#   'attachment-id': '' --> [delete]\n",
    "#   length -> [delete]\n",
    "# ??:\n",
    "# 'namespace': 'we1sv2.0',\n",
    "# 'ppversion': '0.1',\n",
    "\n",
    "# 'name': 'BrazzilLosAngeles_ProQuestDocuments-Humanities-2019-05-08',\n",
    "# 'name': '323722_323722_bournemouthecho_bodypluralartsorhleadpluralarts_2014-01-01_2014-12-31_52_5_0',\n",
    "  \n",
    "# LEXISNEXIS\n",
    "# 'api': 'LexisNexis'\n",
    "# 'attachment-id': '' --> [delete]\n",
    "\n",
    "# 'api': 'Chomp:'\n",
    "# 'api': 'ProQuest'\n",
    "# 'api': 'Reddit'\n",
    "# 'api': 'Twitter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we1s_mongo_import.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"we1s_mongo_import.py\n",
    "Combine imported ZipProcessor and a custom BatchJSONUploader2\n",
    "(prev version was added to ziputils)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pymongo import MongoClient \n",
    "from pymongo.errors import DuplicateKeyError, InvalidDocument\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/home/jovyan/utils/preprocessing/')\n",
    "from libs.fuzzyhasher.fuzzyhasher import FuzzyHasher\n",
    "from libs.zipeditor.zipeditor import ZipEditor, zip_scanner, zip_scanner_excludedirs, ZipProcessor\n",
    "from we1s_utils.ziputils import BatchJSONUploader\n",
    "\n",
    "class BatchJSONUploader2:\n",
    "    \"\"\"Processor takes a file path, iterates over JSON files,\n",
    "    and uploads to a mongodb database.\n",
    "    If a criteria matches, such as being listed in deletes\n",
    "    or a name matching cant or must, then a document is either\n",
    "    inserted (if a target collection is provided) or skipped.\n",
    "    If no rules match, then documents will be uploaded to the\n",
    "    default collection (if provided).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        default_collection,             # 'humanities-keywords'\n",
    "        deletes_collection='deletes',   # 'deletes-humanities'\n",
    "        deletes_file = '_deletes.txt',  # '_deletes.txt'\n",
    "        filter_collection='filter',     #\n",
    "        filter_name_cant='',            # 'no-exact-match'\n",
    "        filter_name_must=''\n",
    "        ):         # 'humanities-keywords-no-exact-match'\n",
    "\n",
    "        self.default_collection = default_collection\n",
    "        self.deletes_collection = deletes_collection\n",
    "        self.deletes_file = deletes_file\n",
    "        self.filter_collection = filter_collection\n",
    "        self.filter_name_cant = filter_name_cant\n",
    "        self.filter_name_must = filter_name_must\n",
    "\n",
    "    def get_json(json_path):\n",
    "        json_data = None\n",
    "        with open(json_path, 'r+') as f:\n",
    "            json_data = json.load(f)\n",
    "            json_data.pop('bag_of_words', None)\n",
    "        return json_data\n",
    "        \n",
    "    def do(self, files_path):\n",
    "        # create delete list\n",
    "        try:\n",
    "            with open(os.path.join(files_path, self.deletes_file), 'r') as f:\n",
    "                self.deletes_list = f.read().splitlines()\n",
    "        except OSError:\n",
    "            self.deletes_list = []\n",
    "        self.json_paths = [os.path.join(r, file) for r, d, f in os.walk(files_path) for file in f if file.endswith('.json') and not file.startswith('._')]\n",
    "        for json_path in self.json_paths:\n",
    "            try:\n",
    "                json_basename = os.path.split(json_path)[1]\n",
    "                if json_basename in self.deletes_list:\n",
    "                    if self.deletes_collection:\n",
    "                        self.deletes_collection.insert_one(get_json(json_path))\n",
    "                elif self.filter_name_must and self.filter_name_must not in json_basename:\n",
    "                    if self.filter_collection:\n",
    "                        self.filter_collection.insert_one(get_json(json_path))\n",
    "                elif self.filter_name_cant and self.filter_name_cant in json_basename:\n",
    "                    if self.filter_collection:\n",
    "                        self.filter_collection.insert_one(get_json(json_path))\n",
    "                elif self.default_collection:\n",
    "                    self.default_collection.insert_one(get_json(json_path))\n",
    "            except (json.decoder.JSONDecodeError, KeyError, PermissionError, ValueError, InvalidDocument) as err:\n",
    "                print('\\n', err.__class__.__name__, \": \", json_path, err)\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mongodb import scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture output\n",
    "\n",
    "print('Import humanities_keywords and reddit')\n",
    "\n",
    "client = MongoClient('mongodb://mongo/')\n",
    "db = client['we1s']\n",
    "\n",
    "upload_list = []\n",
    "\n",
    "hum_zip_path_list = zip_scanner_excludedirs(\n",
    "    source_path='/home/jovyan/data/parsed/humanities-keywords/',\n",
    "    exclude_list=[''], join=True)\n",
    "hum_uploader = BatchJSONUploader2(\n",
    "    default_collection=db['humanities_keywords'],\n",
    "    deletes_file = '_deletes.txt',\n",
    "    deletes_collection=db['deletes_humanities'],\n",
    "    filter_name_cant='no-exact-match',\n",
    "    filter_name_must='',\n",
    "    filter_collection=db['humanities_keywords_no_exact'])\n",
    "upload_list.append(hum_zip_path_list, hum_uploader)\n",
    " \n",
    "rzip_path_list = zip_scanner_excludedirs(\n",
    "    source_path='/home/jovyan/data/parsed/reddit/',\n",
    "    exclude_list=[''], join=True)\n",
    "reddit_uploader = BatchJSONUploader2(\n",
    "    default_collection=db['reddit'],\n",
    "    deletes_file = '_deletes.txt',\n",
    "    deletes_collection=db['deletes_reddit'],\n",
    "    filter_name_cant='',\n",
    "    filter_name_must='',\n",
    "    filter_collection=db['deletes_reddit'])\n",
    "upload_list.append(rzip_path_list, reddit_uploader)\n",
    "\n",
    "for zip_path_list, uploader in scanner_lists:\n",
    "    for zip_path in zip_path_list:\n",
    "        zp = ZipProcessor(zip_path, uploader)\n",
    "        zp.process()\n",
    "        # print('...processed: ', zip_path)\n",
    "        # zp.open()\n",
    "        # x = os.listdir(zp.getdir())\n",
    "        # if '_deletes.txt' in x:\n",
    "        #     print(x)\n",
    "        #     print()\n",
    "        # zp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Import comparison corpus')\n",
    "\n",
    "client = MongoClient('mongodb://mongo/')\n",
    "db = client['we1s']\n",
    "\n",
    "comp_zip_path_list = zip_scanner_excludedirs(source_path='/home/jovyan/data/parsed/comparison-corpus/',\n",
    "                                        exclude_list=[''], join=True)\n",
    "comp_all_uploader = BatchJSONUploader2(\n",
    "    default_collection=db['comparison-not-humanities'],\n",
    "    deletes_file = '_deletes.txt',\n",
    "    deletes_collection=db['deletes_comparison-not-humanities'],\n",
    "    filter_name_cant='',\n",
    "    filter_name_must='no-exact-match',\n",
    "    filter_collection=db['comparison-not-humantiies-filter'])\n",
    "\n",
    "comp_science_uploader = BatchJSONUploader2(\n",
    "    default_collection=db['comparison-sciences'],\n",
    "    deletes_file = '_deletes.txt',\n",
    "    deletes_collection=db['deletes_comparison-sciences'],\n",
    "    filter_name_cant='no-exact-match',\n",
    "    filter_name_must='',\n",
    "    filter_collection=db['comparison-sciences-filter'])\n",
    "\n",
    "# The zips are mixed together in the list, so\n",
    "# two uploaders are used based on the filename:\n",
    "for zip_path in comp_zip_path_list:\n",
    "    if 'humanities_' in zip_path:\n",
    "        zp = ZipProcessor(zip_path, comp_all_uploader)\n",
    "        zp.process()\n",
    "    elif 'sciences_' in zip_path:\n",
    "        zp = ZipProcessor(zip_path, comp_science_uploader)\n",
    "        zp.process()\n",
    "    else:\n",
    "        print('...missed: ', zip_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import csv to mongodb sources, aliases\n",
      "\n",
      "SourcesProcessor(client=MongoClient(host=['mongo:27017'], document_class=dict, tz_aware=False, connect=True), file_path=../sources_master.csv, source_path=['Sources', 'Sources'], aliases_path=['Sources', 'config', 'source_aliases'])\n",
      "aliases: 1644\n",
      "source_docs: 1298\n",
      "csv_parse_log errors: 17\n",
      "Load sources, aliases from mongodb\n",
      "\n",
      "source_docs: 0\n",
      "source_docs: 1298\n",
      "aliases: 0\n",
      "aliases: 1644\n"
     ]
    }
   ],
   "source": [
    "print('Import csv to mongodb sources, aliases\\n')\n",
    "client=MongoClient('mongodb://mongo/')\n",
    "sp = SourcesProcessor(client=MongoClient('mongodb://mongo/'),\n",
    "                      file_path='../sources_master.csv',\n",
    "                      source_path=['Sources','Sources'],\n",
    "                      aliases_path=['Sources','config','source_aliases'])\n",
    "print(sp)\n",
    "sp.get_csv_put_mongo()\n",
    "print('aliases:', len(sp.aliases))\n",
    "print('source_docs:', len(sp.source_docs))\n",
    "print('csv_parse_log errors:', len(sp.csv_parse_log))\n",
    "# view errors:\n",
    "# print('\\n'.join(sp.csv_parse_log))\n",
    "\n",
    "print('Load sources, aliases from mongodb\\n')\n",
    "\n",
    "sp.clear()\n",
    "\n",
    "print('source_docs:', len(sp.source_docs))\n",
    "sp.get_mongo_source_docs()\n",
    "print('source_docs:', len(sp.source_docs))\n",
    "\n",
    "print('aliases:', len(sp.aliases))\n",
    "sp.get_mongo_aliases()\n",
    "print('aliases:', len(sp.aliases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup ArticleProcessor with SourcesProcessor\n"
     ]
    }
   ],
   "source": [
    "print('Setup ArticleProcessor with SourcesProcessor')\n",
    "client=MongoClient('mongodb://mongo/')\n",
    "sp = SourcesProcessor(client=MongoClient('mongodb://mongo/'),\n",
    "                      file_path='../sources_master.csv',\n",
    "                      source_path=['Sources','Sources'],\n",
    "                      aliases_path=['Sources','config','source_aliases'])\n",
    "ap = ArticleProcessor(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewrite documents in memory and preview in-memory, no mongo update\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>td{border: 1px solid black} td{text-align:left; vertical-align:top}</style><table><tr style><td><pre>before (preview)</pre></td><td><pre>after (preview)</pre></td></tr><tr><td><pre>{'_id': ObjectId('5d3ada4af123b8357f4c3ed5'),\n",
       " 'attachment_id': 'LNCDBE032A334E6199AEB6E6EFA...',\n",
       " 'author': 'Philip Sweeney',\n",
       " 'content': 'Fans of the work of Vincent Van Gogh, of '\n",
       "            'industrial heritage and of vintage Hollywood '\n",
       "            'have a treat waiting in the European Capital '\n",
       "            'of Culture program of Mons in Belgium this '\n",
       "            'spring: all three subjects in one intriguing '\n",
       "            'package of exhibitions, screenings and darkly '\n",
       "            'atmospheric topography. The atmospheric '\n",
       "            'destination is the Borinage, the impoverished '\n",
       "            'region around Mons, where long rows of grimy '\n",
       "            \"workers' terraces and the remains of railways \"\n",
       "            'and canals stretch around a landscape of low '\n",
       "            'hills. Thi...',\n",
       " 'content-hash-ssdeep': '96:tBRCddiM2SXsGf4ZSRw6ESTV...',\n",
       " 'copyright': 'All Rights Reserved',\n",
       " 'database': 'LexisNexis',\n",
       " 'doc_id': '02A6A252C52394AB97B14672E56C2F2F91752...',\n",
       " 'length': '1086',\n",
       " 'metapath': 'Corpus,lexisnexis,8200_8200_theindepe...',\n",
       " 'name': '8200_8200_theindependentunitedkingdom...',\n",
       " 'namespace': 'we1sv2.0',\n",
       " 'ppversion': '0.1',\n",
       " 'pub': None,\n",
       " 'pub_date': '2015-02-17T18:51:00Z',\n",
       " 'readability_scores': [45.95332522187235,\n",
       "                        11.90957793137295,\n",
       "                        9.994435839024987],\n",
       " 'section': 'EUROPE',\n",
       " 'sources': ['theindependentunitedkingdom'],\n",
       " 'title': None,\n",
       " 'word_count': 1009}</pre></td><td><pre>{'_id': ObjectId('5d3ada4af123b8357f4c3ed5'),\n",
       " 'api_data_provider': 'LexisNexis',\n",
       " 'api_software': 'we1s-collector',\n",
       " 'attachment_id': 'LNCDBE032A334E6199AEB6E6EFA...',\n",
       " 'author': 'Philip Sweeney',\n",
       " 'content': 'Fans of the work of Vincent Van Gogh, of '\n",
       "            'industrial heritage and of vintage Hollywood '\n",
       "            'have a treat waiting in the European Capital '\n",
       "            'of Culture program of Mons in Belgium this '\n",
       "            'spring: all three subjects in one intriguing '\n",
       "            'package of exhibitions, screenings and darkly '\n",
       "            'atmospheric topography. The atmospheric '\n",
       "            'destination is the Borinage, the impoverished '\n",
       "            'region around Mons, where long rows of grimy '\n",
       "            \"workers' terraces and the remains of railways \"\n",
       "            'and canals stretch around a landscape of low '\n",
       "            'hills. Thi...',\n",
       " 'content-hash-ssdeep': '96:tBRCddiM2SXsGf4ZSRw6ESTV...',\n",
       " 'copyright': 'All Rights Reserved',\n",
       " 'database': 'LexisNexis',\n",
       " 'doc_id': '02A6A252C52394AB97B14672E56C2F2F91752...',\n",
       " 'length': '1086',\n",
       " 'metapath': 'Corpus,lexisnexis,8200_8200_theindepe...',\n",
       " 'name': '8200_8200_theindependentunitedkingdom...',\n",
       " 'namespace': 'we1sv2.0',\n",
       " 'ppversion': '0.1',\n",
       " 'pub': None,\n",
       " 'pub_date': '2015-02-17T18:51:00Z',\n",
       " 'readability_scores': [45.95332522187235,\n",
       "                        11.90957793137295,\n",
       "                        9.994435839024987],\n",
       " 'section': 'EUROPE',\n",
       " 'source': 'the-independent-united-kingdom',\n",
       " 'title': None,\n",
       " 'word_count': 1009}</pre></td></tr><tr><td><pre>{'_id': ObjectId('5d3ac54af123b8357f469437'),\n",
       " 'attachment_id': '...',\n",
       " 'author': 'Gracie DiFazio',\n",
       " 'content': 'The CEO of Bottega Veneta, Carlo Alberto '\n",
       "            'Beretta, gave a lecture on Tradition, '\n",
       "            'Elegance and Craftsmanship: The Timeless '\n",
       "            'Luxury Model of Bottega Veneta in Bartley '\n",
       "            'Hall last Tuesday at 4:30 p. m. This lecture '\n",
       "            'was an event in the lecture series on Italian '\n",
       "            'Business entitled: The Difference Italy '\n",
       "            'Makes. Bottega Veneta is a luxury brand '\n",
       "            'produced and made in the Veneto region of '\n",
       "            'Italy. Their signature woven design acts in '\n",
       "            'place of a logo on their bags. \"When your own '\n",
       "            'initials are enough\" has been th...',\n",
       " 'content-hash-ssdeep': '96:t2MkmZkJ/OSvFYYYYYYYYYYY...',\n",
       " 'copyright': 'All Rights Reserved',\n",
       " 'database': 'LexisNexis',\n",
       " 'doc_id': '02A6A252C52394AB97B14672E56C2F2F37435...',\n",
       " 'length': '687',\n",
       " 'metapath': 'Corpus,lexisnexis,172244_172244_unive...',\n",
       " 'name': '172244_172244_universitywire_bodylibe...',\n",
       " 'namespace': 'we1sv2.0',\n",
       " 'ppversion': '0.1',\n",
       " 'pub': 'The Villanovan: Villanova University',\n",
       " 'pub_date': '2015-11-04T00:00:00Z',\n",
       " 'readability_scores': [47.67224534501645,\n",
       "                        10.359008762322016,\n",
       "                        8.427629572836803],\n",
       " 'section': 'NEWS; Pg. 1',\n",
       " 'sources': ['universitywire'],\n",
       " 'title': 'CEO brings Italian luxury to campus',\n",
       " 'word_count': 666}</pre></td><td><pre>{'_id': ObjectId('5d3ac54af123b8357f469437'),\n",
       " 'api_data_provider': 'LexisNexis',\n",
       " 'api_software': 'we1s-collector',\n",
       " 'attachment_id': '...',\n",
       " 'author': 'Gracie DiFazio',\n",
       " 'content': 'The CEO of Bottega Veneta, Carlo Alberto '\n",
       "            'Beretta, gave a lecture on Tradition, '\n",
       "            'Elegance and Craftsmanship: The Timeless '\n",
       "            'Luxury Model of Bottega Veneta in Bartley '\n",
       "            'Hall last Tuesday at 4:30 p. m. This lecture '\n",
       "            'was an event in the lecture series on Italian '\n",
       "            'Business entitled: The Difference Italy '\n",
       "            'Makes. Bottega Veneta is a luxury brand '\n",
       "            'produced and made in the Veneto region of '\n",
       "            'Italy. Their signature woven design acts in '\n",
       "            'place of a logo on their bags. \"When your own '\n",
       "            'initials are enough\" has been th...',\n",
       " 'content-hash-ssdeep': '96:t2MkmZkJ/OSvFYYYYYYYYYYY...',\n",
       " 'copyright': 'All Rights Reserved',\n",
       " 'database': 'LexisNexis',\n",
       " 'doc_id': '02A6A252C52394AB97B14672E56C2F2F37435...',\n",
       " 'length': '687',\n",
       " 'metapath': 'Corpus,lexisnexis,172244_172244_unive...',\n",
       " 'name': '172244_172244_universitywire_bodylibe...',\n",
       " 'namespace': 'we1sv2.0',\n",
       " 'ppversion': '0.1',\n",
       " 'pub': 'The Villanovan: Villanova University',\n",
       " 'pub_date': '2015-11-04T00:00:00Z',\n",
       " 'readability_scores': [47.67224534501645,\n",
       "                        10.359008762322016,\n",
       "                        8.427629572836803],\n",
       " 'section': 'NEWS; Pg. 1',\n",
       " 'source': 'the-villanovan-villanova-university',\n",
       " 'title': 'CEO brings Italian luxury to campus',\n",
       " 'word_count': 666}</pre></td></tr><tr><td><pre>{'_id': ObjectId('5d3ac989f123b8357f479cb8'),\n",
       " 'attachment_id': '...',\n",
       " 'author': 'By, Maria Polletta',\n",
       " 'content': '\"We have these new schools, but we also had '\n",
       "            '40,000 college students before we started '\n",
       "            'this. Every year, we step back and evaluate '\n",
       "            \"whether we're effective in partnerships with \"\n",
       "            'them or if there[.] something else they need '\n",
       "            'us to do.\" Bill Jabjiniak, Mesa[.] director '\n",
       "            'of economic development Three years after a '\n",
       "            'Mesa push to attract old-school liberal - '\n",
       "            'arts universities landed the city five new '\n",
       "            'branch colleges, two of them have cut their '\n",
       "            'losses and decided to leave town. '\n",
       "            'Missouri-based Westminster...',\n",
       " 'content-hash-ssdeep': '192:6gVBYOR58DNVBWyYYYYYYYY...',\n",
       " 'copyright': 'All Rights Reserved',\n",
       " 'database': 'LexisNexis',\n",
       " 'doc_id': '02A6A252C52394AB97B14672E56C2F2F9EAE1...',\n",
       " 'length': '1324',\n",
       " 'metapath': 'Corpus,lexisnexis,286699_286699_thear...',\n",
       " 'name': '286699_286699_thearizonarepublicphoen...',\n",
       " 'namespace': 'we1sv2.0',\n",
       " 'ppversion': '0.1',\n",
       " 'pub': 'The Arizona Republic (Phoenix)',\n",
       " 'pub_date': '2015-08-05T00:00:00Z',\n",
       " 'readability_scores': [44.13776035253713,\n",
       "                        10.343478156216154,\n",
       "                        9.564550143958822],\n",
       " 'section': 'SCOTTSDALE REPUBLIC 8; Pg. Z812',\n",
       " 'sources': ['thearizonarepublicphoenix'],\n",
       " 'title': 'Do branch colleges have what it takes to thrive '\n",
       "          'in downtown Mesa?',\n",
       " 'word_count': 1192}</pre></td><td><pre>{'_id': ObjectId('5d3ac989f123b8357f479cb8'),\n",
       " 'api_data_provider': 'LexisNexis',\n",
       " 'api_software': 'we1s-collector',\n",
       " 'attachment_id': '...',\n",
       " 'author': 'By, Maria Polletta',\n",
       " 'content': '\"We have these new schools, but we also had '\n",
       "            '40,000 college students before we started '\n",
       "            'this. Every year, we step back and evaluate '\n",
       "            \"whether we're effective in partnerships with \"\n",
       "            'them or if there[.] something else they need '\n",
       "            'us to do.\" Bill Jabjiniak, Mesa[.] director '\n",
       "            'of economic development Three years after a '\n",
       "            'Mesa push to attract old-school liberal - '\n",
       "            'arts universities landed the city five new '\n",
       "            'branch colleges, two of them have cut their '\n",
       "            'losses and decided to leave town. '\n",
       "            'Missouri-based Westminster...',\n",
       " 'content-hash-ssdeep': '192:6gVBYOR58DNVBWyYYYYYYYY...',\n",
       " 'copyright': 'All Rights Reserved',\n",
       " 'database': 'LexisNexis',\n",
       " 'doc_id': '02A6A252C52394AB97B14672E56C2F2F9EAE1...',\n",
       " 'length': '1324',\n",
       " 'metapath': 'Corpus,lexisnexis,286699_286699_thear...',\n",
       " 'name': '286699_286699_thearizonarepublicphoen...',\n",
       " 'namespace': 'we1sv2.0',\n",
       " 'ppversion': '0.1',\n",
       " 'pub': 'The Arizona Republic (Phoenix)',\n",
       " 'pub_date': '2015-08-05T00:00:00Z',\n",
       " 'readability_scores': [44.13776035253713,\n",
       "                        10.343478156216154,\n",
       "                        9.564550143958822],\n",
       " 'section': 'SCOTTSDALE REPUBLIC 8; Pg. Z812',\n",
       " 'source': 'the-arizona-republic-phoenix',\n",
       " 'title': 'Do branch colleges have what it takes to thrive '\n",
       "          'in downtown Mesa?',\n",
       " 'word_count': 1192}</pre></td></tr><tr><td><pre>{'_id': ObjectId('5d3ad901f123b8357f4be792'),\n",
       " 'attachment_id': 'LNCDBE032A334E6199591459EC4...',\n",
       " 'author': 'Gordon Rayner',\n",
       " 'content': 'AN MI6 spy found dead inside a padlocked '\n",
       "            'holdall could have been killed by someone who '\n",
       "            'specialised in \"the dark arts of the secret '\n",
       "            'services,\" a coroner was told yesterday. '\n",
       "            'Gareth Williams could not have locked the bag '\n",
       "            'from the inside, meaning a \"third party\" must '\n",
       "            'have done it, according to a lawyer '\n",
       "            'representing his family. They believe his '\n",
       "            'death in 2010 may have been linked to his '\n",
       "            'work at MI6, where he had recently qualified '\n",
       "            'for \"operational deployment\", and that '\n",
       "            'fingerprints, DNA and other e...',\n",
       " 'content-hash-ssdeep': '96:+WQPuPPPE4uvLW6paOXtuht/...',\n",
       " 'copyright': 'All Rights Reserved',\n",
       " 'database': 'LexisNexis',\n",
       " 'doc_id': '02A6A252C52394AB97B14672E56C2F2F32372...',\n",
       " 'length': '1068',\n",
       " 'metapath': 'Corpus,lexisnexis,8109_8109_thedailyt...',\n",
       " 'name': '8109_8109_thedailytelegraph_bodyplura...',\n",
       " 'namespace': 'we1sv2.0',\n",
       " 'ppversion': '0.1',\n",
       " 'pub': None,\n",
       " 'pub_date': '2012-03-31T00:00:00Z',\n",
       " 'readability_scores': [53.594220623501215,\n",
       "                        12.361433710174719,\n",
       "                        8.414431277834876],\n",
       " 'section': 'NEWS; Pg. 3',\n",
       " 'sources': ['thedailytelegraph'],\n",
       " 'title': None,\n",
       " 'word_count': 1120}</pre></td><td><pre>{'_id': ObjectId('5d3ad901f123b8357f4be792'),\n",
       " 'api_data_provider': 'LexisNexis',\n",
       " 'api_software': 'we1s-collector',\n",
       " 'attachment_id': 'LNCDBE032A334E6199591459EC4...',\n",
       " 'author': 'Gordon Rayner',\n",
       " 'content': 'AN MI6 spy found dead inside a padlocked '\n",
       "            'holdall could have been killed by someone who '\n",
       "            'specialised in \"the dark arts of the secret '\n",
       "            'services,\" a coroner was told yesterday. '\n",
       "            'Gareth Williams could not have locked the bag '\n",
       "            'from the inside, meaning a \"third party\" must '\n",
       "            'have done it, according to a lawyer '\n",
       "            'representing his family. They believe his '\n",
       "            'death in 2010 may have been linked to his '\n",
       "            'work at MI6, where he had recently qualified '\n",
       "            'for \"operational deployment\", and that '\n",
       "            'fingerprints, DNA and other e...',\n",
       " 'content-hash-ssdeep': '96:+WQPuPPPE4uvLW6paOXtuht/...',\n",
       " 'copyright': 'All Rights Reserved',\n",
       " 'database': 'LexisNexis',\n",
       " 'doc_id': '02A6A252C52394AB97B14672E56C2F2F32372...',\n",
       " 'length': '1068',\n",
       " 'metapath': 'Corpus,lexisnexis,8109_8109_thedailyt...',\n",
       " 'name': '8109_8109_thedailytelegraph_bodyplura...',\n",
       " 'namespace': 'we1sv2.0',\n",
       " 'ppversion': '0.1',\n",
       " 'pub': None,\n",
       " 'pub_date': '2012-03-31T00:00:00Z',\n",
       " 'readability_scores': [53.594220623501215,\n",
       "                        12.361433710174719,\n",
       "                        8.414431277834876],\n",
       " 'section': 'NEWS; Pg. 3',\n",
       " 'source': 'the-daily-telegraph',\n",
       " 'title': None,\n",
       " 'word_count': 1120}</pre></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "isolated": true
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Rewrite documents in memory and preview in-memory, no mongo update')\n",
    "\n",
    "# select data\n",
    "coll = client['we1s']['deletes_humanities']\n",
    "docs = coll.aggregate([{ '$sample': { 'size': 4 } }])\n",
    "\n",
    "# preview config\n",
    "pop_list = ['features','language_model','content-unscrubbed']\n",
    "trim_dict={'content':500, 'attachment_id': 30, 'doc_id':40, 'metapath':40, 'content-hash-ssdeep': 30, 'name':40}\n",
    "\n",
    "# display with single table style\n",
    "data = []\n",
    "data.append(('before (preview)', 'after (preview)'))\n",
    "for doc in docs:\n",
    "    before, after = ap.json_update_previews(doc, pop_list=pop_list, trim_dict=trim_dict, width=60)\n",
    "    # the full `doc` has been changed at this point, could be written.\n",
    "    data.append((before,after))\n",
    "ap.display_ipython_table(data) \n",
    "\n",
    "# # ...or display with floating table rows style\n",
    "# for doc in docs:\n",
    "#     before, after = ap.json_update_previews(doc, pop_list=pop_list, trim_dict=trim_dict, width=60)\n",
    "#     # the full `doc` has been changed at this point, could be written.\n",
    "#     ap.display_ipython_table([(before,after)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From-mongo rewrite w/out update -- could instead rewrite before insert\n",
    "\n",
    "# # def rewrite_docs(aliases, docs):\n",
    "# for doc in docs: \n",
    "#     rewrite(doc, aliases)\n",
    "#     print(doc_preview(doc, pop_list=['features','language_model','content-unscrubbed'], trim_dict={'content':500}))\n",
    "\n",
    "# # def rewrite_mongo_coll(aliases, coll, docs):\n",
    "# for doc in docs: \n",
    "#     rewrite(doc)\n",
    "#     coll.replace_one({'_id':doc['_id']}, doc, upsert=True)\n",
    "#     print(doc_preview(doc, pop_list=['features','language_model','content-unscrubbed'], trim_dict={'content':500})\n",
    "\n",
    "# aliases = client['Sources']['config'].find_one({'_id':'source_aliases'})['aliases']\n",
    "# coll = client['we1s']['deletes_humanities']\n",
    "# docs = coll.aggregate([{ '$sample': { 'size': 2 } }])\n",
    "# rewrite_mongo_coll(aliases, coll, docs)\n",
    "\n",
    "# docs = client['we1s']['deletes_humanities'].aggregate([{ '$sample': { 'size': 2 } }])\n",
    "# # docs = client['we1s']['deletes_humanities'].collection.find({})\n",
    "\n",
    "# OUTPUT:\n",
    "# rewrite: 5d3acd40f123b8357f488425\n",
    "# { '_id': ObjectId('5d3acd40f123b8357f488425'),\n",
    "#   'api_data_provider': 'LexisNexis',\n",
    "#   'api_software': 'we1s-collector',\n",
    "#   'attachment_id': 'LNCDBE032A334E6199C0814D4A55AE10E64CD13C619E192CC1',\n",
    "#   'content': 'She has already penned poems in honour of the last British '\n",
    "#              'soldiers to fight in the First World War, the wedding of Prince '\n",
    "#              'William and Kate Middleton, the MPs expenses scandal and an '\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports: Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources counts in articles collection (descending)\n",
      "{'_id': ['thenewyorktimes'], 'count': 57700}\n",
      "{'_id': ['thewashingtonpost'], 'count': 37306}\n",
      "{'_id': ['thelatimes'], 'count': 34483}\n",
      "{'_id': ['chicagotribune'], 'count': 25541}\n",
      "{'_id': ['universitywire'], 'count': 24459}\n",
      "{'_id': ['theirishtimes'], 'count': 12741}\n",
      "{'_id': ['newsday'], 'count': 9656}\n",
      "{'_id': ['deseretmorningnewssaltlakecity'], 'count': 6496}\n",
      "{'_id': ['theguardianlondon'], 'count': 6414}\n",
      "{'_id': ['arkansasdemocratgazette'], 'count': 6231}\n",
      "{'_id': ['mirrorcouk'], 'count': 6136}\n",
      "{'_id': ['thehoustonchronicle'], 'count': 6071}\n",
      "{'_id': ['tveyesbbcradio4'], 'count': 5651}\n",
      "{'_id': ['tveyesbbcworld'], 'count': 5078}\n",
      "{'_id': ['targetednewsservice'], 'count': 5052}\n",
      "{'_id': ['globalbroadcastdatabase'], 'count': 4813}\n",
      "{'_id': ['bbcmonitoringinternationalreports'], 'count': 4538}\n",
      "{'_id': ['theindependentunitedkingdom'], 'count': 4291}\n",
      "{'_id': ['tveyesbbcradio5live'], 'count': 3401}\n",
      "{'_id': ['chronicleofhighereducation'], 'count': 3300}\n",
      "{'_id': ['thehartfordcourant'], 'count': 2836}\n",
      "{'_id': ['irishindependent'], 'count': 2769}\n",
      "{'_id': ['thedailytelegraph'], 'count': 2621}\n",
      "{'_id': ['thebostonglobe'], 'count': 2360}\n",
      "{'_id': None, 'count': 2333}\n",
      "{'_id': ['channelnewsasia'], 'count': 2191}\n",
      "{'_id': ['tveyesbbcradio2'], 'count': 2001}\n",
      "{'_id': ['tveyesbbc1northernireland'], 'count': 1925}\n",
      "{'_id': ['belfasttelegraph'], 'count': 1897}\n",
      "{'_id': ['tveyesbbc1wales'], 'count': 1873}\n",
      "{'_id': ['usatoday'], 'count': 1864}\n",
      "{'_id': ['argusleadersouthdakota'], 'count': 1786}\n",
      "{'_id': ['federalnewsservice'], 'count': 1719}\n",
      "{'_id': ['tveyesbbc1east'], 'count': 1702}\n",
      "{'_id': ['tveyesbbc1london'], 'count': 1699}\n",
      "{'_id': ['tveyesbbc1eastmidlands'], 'count': 1690}\n",
      "{'_id': ['theeveningstandardlondon'], 'count': 1680}\n",
      "{'_id': ['tveyesbbc1northwest'], 'count': 1677}\n",
      "{'_id': ['stlouispostdispatch'], 'count': 1652}\n",
      "{'_id': ['tveyesbbc1north'], 'count': 1649}\n",
      "{'_id': ['tveyesbbc2scotland'], 'count': 1616}\n",
      "{'_id': ['thebirminghamnews'], 'count': 1616}\n",
      "{'_id': ['tveyesbbc2'], 'count': 1609}\n",
      "{'_id': ['tveyesbbc1west'], 'count': 1588}\n",
      "{'_id': ['tveyesbbc1southwest'], 'count': 1577}\n",
      "{'_id': ['tveyesbbc1oxford'], 'count': 1573}\n",
      "{'_id': ['theoregonian'], 'count': 1552}\n",
      "{'_id': ['thedallasmorningnews'], 'count': 1551}\n",
      "{'_id': ['tveyesbbc1southeast'], 'count': 1547}\n",
      "{'_id': ['tveyesbbc1southampton'], 'count': 1528}\n",
      "{'_id': ['tveyesbbc1northeastandcumbria'], 'count': 1520}\n",
      "{'_id': ['tveyesbbc1yorkshireandlincolnshire'], 'count': 1510}\n",
      "{'_id': ['theadvocatebatonrouge'], 'count': 1499}\n",
      "{'_id': ['tveyesbbc1scotland'], 'count': 1493}\n",
      "{'_id': ['dailynews'], 'count': 1492}\n",
      "{'_id': ['scotsman'], 'count': 1320}\n",
      "{'_id': ['thedailyoklahomanoklahomacity'], 'count': 1318}\n",
      "{'_id': ['thewallstreetjournal'], 'count': 1299}\n",
      "{'_id': ['theknoxvillenewssentinel'], 'count': 1299}\n",
      "{'_id': ['theheraldglasgow'], 'count': 1291}\n",
      "{'_id': ['tveyesbbc2wales'], 'count': 1257}\n",
      "{'_id': ['thestarledgernewark'], 'count': 1248}\n",
      "{'_id': ['tveyesbbcradio1'], 'count': 1240}\n",
      "{'_id': ['elnuevoherald'], 'count': 1218}\n",
      "{'_id': ['tveyesbbcnews24'], 'count': 1180}\n",
      "{'_id': ['chicagodailyherald'], 'count': 1174}\n",
      "{'_id': ['easterndailypress'], 'count': 1135}\n",
      "{'_id': ['startribuneminneapolis'], 'count': 1124}\n",
      "{'_id': ['liverpoolecho'], 'count': 1081}\n",
      "{'_id': ['providencejournal'], 'count': 1080}\n",
      "{'_id': ['thecharlotteobserver'], 'count': 1057}\n",
      "{'_id': ['iindependentprintltd'], 'count': 1043}\n",
      "{'_id': ['bristolpost'], 'count': 1032}\n",
      "{'_id': ['irishnews'], 'count': 1027}\n",
      "{'_id': ['thealbuquerquejournal'], 'count': 1014}\n",
      "{'_id': ['dailystar'], 'count': 1006}\n",
      "{'_id': ['tveyesbbc4'], 'count': 991}\n",
      "{'_id': ['thedailymailandmailonsunday'], 'count': 975}\n",
      "{'_id': ['thephiladelphiainquirer'], 'count': 945}\n",
      "{'_id': ['thearizonarepublicphoenix'], 'count': 944}\n",
      "{'_id': ['theobserverlondon'], 'count': 907}\n",
      "{'_id': ['newyorkpost'], 'count': 906}\n",
      "{'_id': ['cnncom'], 'count': 897}\n",
      "{'_id': ['chicagodefender'], 'count': 891}\n",
      "{'_id': ['alaskadispatchnews'], 'count': 885}\n",
      "{'_id': ['newsweek'], 'count': 882}\n",
      "{'_id': ['thenorthernecho'], 'count': 853}\n",
      "{'_id': ['bismarcktribune'], 'count': 842}\n",
      "{'_id': ['lexingtonheraldleader'], 'count': 841}\n",
      "{'_id': ['theindianapolisstar'], 'count': 810}\n",
      "{'_id': ['thesaltlaketribune'], 'count': 799}\n",
      "{'_id': ['southwaleseveningpost'], 'count': 795}\n",
      "{'_id': ['cqcongressionaltestimony'], 'count': 778}\n",
      "{'_id': ['dailypostnorthwales'], 'count': 770}\n",
      "{'_id': ['omahaworldherald'], 'count': 738}\n",
      "{'_id': ['tveyesbbcparliament'], 'count': 712}\n",
      "{'_id': ['nottinghampost'], 'count': 712}\n",
      "{'_id': ['leicestermercury'], 'count': 702}\n",
      "{'_id': ['thedesmoinesregister'], 'count': 694}\n",
      "{'_id': ['thebaltimoresun'], 'count': 674}\n",
      "{'_id': ['daytondailynewsohio'], 'count': 671}\n",
      "CPU times: user 55.5 ms, sys: 19.1 ms, total: 74.5 ms\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ~1 min\n",
    "# list source by article counts, descending\n",
    "# NOTE: source is distinct from pub (publications)\n",
    "\n",
    "print('Sources counts in articles collection (descending)')\n",
    "client = MongoClient('mongodb://mongo/')\n",
    "pipeline = [\n",
    "    {'$group' : {'_id' : '$sources', 'count' : {'$sum' : 1}}},\n",
    "    { '$sort' : {'count' : -1} }\n",
    "]\n",
    "result = client['we1s'].command('aggregate', 'humanities_keywords', pipeline=pipeline, explain=False)\n",
    "for row in result['cursor']['firstBatch']:\n",
    "    print(row)\n",
    "    \n",
    "# OUTPUT\n",
    "# {'_id': ['thenewyorktimes'], 'count': 57700}\n",
    "# {'_id': ['thewashingtonpost'], 'count': 37306}\n",
    "# {'_id': ['thelatimes'], 'count': 34483}\n",
    "# {'_id': ['chicagotribune'], 'count': 25541}\n",
    "# {'_id': ['universitywire'], 'count': 24459}\n",
    "# {'_id': ['theirishtimes'], 'count': 12741}\n",
    "# ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports: Documents\n",
    "\n",
    "Display document counts (or other collstats) across a database, or all databases on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count collection documents in one database\n",
      "\n",
      "count       db          coll        \n",
      "752243      we1s        reddit      \n",
      "508490      we1s        humanities_keywords_no_exact\n",
      "12          we1s        comparison-not-humantiies-filter\n",
      "418302      we1s        humanities_keywords\n",
      "6607        we1s        comparison-sciences-filter\n",
      "628317      we1s        comparison-sciences\n",
      "111396      we1s        deletes_humanities\n",
      "635495      we1s        comparison-not-humanities\n",
      "47320       we1s        deletes_reddit\n",
      "44114       we1s        deletes_comparison-not-humanities\n",
      "66612       we1s        deletes_comparison-sciences\n",
      "1           we1s        _config     \n",
      "\n",
      "Size collection documents in one datanbase\n",
      "\n",
      "avgObjSize  db          coll        \n",
      "19788       we1s        reddit      \n",
      "105391      we1s        humanities_keywords_no_exact\n",
      "475810      we1s        comparison-not-humantiies-filter\n",
      "137407      we1s        humanities_keywords\n",
      "61186       we1s        comparison-sciences-filter\n",
      "126635      we1s        comparison-sciences\n",
      "112806      we1s        deletes_humanities\n",
      "110219      we1s        comparison-not-humanities\n",
      "23357       we1s        deletes_reddit\n",
      "107897      we1s        deletes_comparison-not-humanities\n",
      "116395      we1s        deletes_comparison-sciences\n",
      "105650      we1s        _config     \n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # fast\n",
    "\n",
    "client=MongoClient('mongodb://mongo/')\n",
    "\n",
    "print(\"\\nCount collection documents in one database\\n\")\n",
    "report = report_collstats(db_colls(client, 'we1s'), key='count')\n",
    "print_table(report)\n",
    "\n",
    "print(\"\\nSize collection documents in one datanbase\\n\")\n",
    "report = report_collstats(db_colls(client, 'we1s'), key='avgObjSize')\n",
    "print_table(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count collection documents in each db\n",
      "\n",
      "count       db          coll        \n",
      "1298        Sources     Sources     \n",
      "1           admin       system.version\n",
      "1           app         apps        \n",
      "0           app         streams     \n",
      "1           app         metadata    \n",
      "240         app         deployments \n",
      "0           app         drafts      \n",
      "2           auth        passwords   \n",
      "10          auth        devices     \n",
      "1           auth        groups      \n",
      "0           auth        pushMessages\n",
      "95          auth        refreshTokens\n",
      "0           auth        apiKeys     \n",
      "2           auth        users       \n",
      "26          config      system.sessions\n",
      "0           events      unordered_queue\n",
      "1           events      counters    \n",
      "1           events      ordered_queue\n",
      "0           hosting     assets      \n",
      "0           hosting     usage       \n",
      "80          local       startup_log \n",
      "664         log         log         \n",
      "0           metadata    settings    \n",
      "1           metadata    dashboards  \n",
      "1           metadata    items       \n",
      "16          metadata    datasources \n",
      "1           metadata    users       \n",
      "752243      we1s        reddit      \n",
      "508490      we1s        humanities_keywords_no_exact\n",
      "12          we1s        comparison-not-humantiies-filter\n",
      "418302      we1s        humanities_keywords\n",
      "6607        we1s        comparison-sciences-filter\n",
      "628317      we1s        comparison-sciences\n",
      "111396      we1s        deletes_humanities\n",
      "635495      we1s        comparison-not-humanities\n",
      "47320       we1s        deletes_reddit\n",
      "44114       we1s        deletes_comparison-not-humanities\n",
      "66612       we1s        deletes_comparison-sciences\n",
      "1           we1s        _config     \n",
      "0           we1s2018    Sources     \n",
      "548329      we1s2018    Corpus      \n",
      "1           we1s2018    testcollection\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # fast\n",
    "\n",
    "client=MongoClient('mongodb://mongo/')\n",
    "\n",
    "print(\"\\nCount collection documents in each db\\n\")\n",
    "report = report_collstats(client_colls(client), key='count')\n",
    "print_table(report)\n",
    "\n",
    "\n",
    "# DEPRECATED\n",
    "# print(\"Count documents in each db collection\\n\")\n",
    "# mc = MongoCounter(client=MongoClient('mongodb://mongo/'))\n",
    "# mc.count_docs_report()\n",
    "\n",
    "\n",
    "# # DEPRECATED\n",
    "# # list records in all collections\n",
    "# d = dict((db, [collection for collection in client[db].list_collection_names()])\n",
    "#              for db in client.list_database_names())\n",
    "# for db in d:\n",
    "#     for coll in d[db]:\n",
    "#         # print(db, coll)\n",
    "#         print((client[db].command(\"collstats\", coll))['count'], db, coll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manipulate collection documents report as data\n",
      "\n",
      "Documents across collections:\n",
      "3769681\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # fast\n",
    "\n",
    "client=MongoClient('mongodb://mongo/')\n",
    "\n",
    "print(\"\\nManipulate collection documents report as data\\n\")\n",
    "report = report_collstats(client_colls(client), key='count', header=False)\n",
    "s = sum(row[0] for row in report)\n",
    "print('Documents across collections:')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports: Fields\n",
    "\n",
    "Summarize counts of field presence/absence in a collection, in all collections in a database,\n",
    "or across all databases on the server.\n",
    "\n",
    "Some other possibly approaches --\n",
    "including server-side map-reduce and a tool valled Variety --\n",
    "are discussed here:\n",
    "\n",
    "-  https://stackoverflow.com/questions/2298870/get-names-of-all-keys-in-the-collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display distinct values for one field in a collection.\n",
      "CPU times: user 41.6 ms, sys: 8.78 ms, total: 50.4 ms\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## SLOW -- ~1 minute\n",
    "print(\"Display distinct values for one field in a collection.\")\n",
    "client['we1s']['humanities_keywords'].distinct('term')\n",
    "\n",
    "# OUTPUT:\n",
    "# ['LexisNexis',\n",
    "#  'chomp',\n",
    "#  'Ethnic NewsWatch',\n",
    "#  'Ethnic NewsWatch; GenderWatch',\n",
    "#  'ProQuest',\n",
    "#  'GenderWatch',\n",
    "#  'Ethnic NewsWatc h',\n",
    "#  'Ethnic N ewsWatch',\n",
    "#  'Global Newsstream',\n",
    "#  'Global Newsstrea m',\n",
    "#  'Globa l Newsstream',\n",
    "#  'Global Newss tream']\n",
    "\n",
    "# client['we1s']['humanities_keywords'].distinct('term')\n",
    "\n",
    "# OUTPUT:\n",
    "# ['humanities', ' humanities', ' liberal_arts']\n",
    "\n",
    "# OTHER POSSIBLE LOOKUPS:\n",
    "#     client['we1s']['humanities_keywords'].distinct('api_data_provider')\n",
    "#     client['we1s']['humanities_keywords'].distinct('api_data_provider_channel')\n",
    "#     client['we1s']['humanities_keywords'].distinct('api_software')\n",
    "#     client['we1s']['humanities_keywords'].distinct('database')\n",
    "#     client['we1s']['humanities_keywords'].distinct('term')\n",
    "\n",
    "# def store_suggested_lookups():\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "list publications by article counts, descending\n",
      "\n",
      "\"cursor\"\n",
      "\"ok\"\n",
      "CPU times: user 94 ms, sys: 29.7 ms, total: 124 ms\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## SLOW\n",
    "\n",
    "print('\\nlist publications by article counts, descending\\n')\n",
    "\n",
    "client=MongoClient('mongodb://mongo/')\n",
    "coll_list = [(client, 'we1s', 'humanities_keywords')]\n",
    "pipeline = [\n",
    "    {'$group' : {'_id' : '$pub', 'count' : {'$sum' : 1}}},\n",
    "    { '$sort' : {'count' : -1} }\n",
    "]\n",
    "result = report_aggregate(coll_list, pipeline)\n",
    "for row in result:\n",
    "    print(row)\n",
    "# for row in result['cursor']['firstBatch']:\n",
    "#     if row:\n",
    "#         print(row['count'], '\\t', row['_id'])\n",
    "\n",
    "# ## DEPRECATED\n",
    "# # list publications by article counts, descending\n",
    "# client=MongoClient('mongodb://mongo/')\n",
    "# pipeline = [\n",
    "#     {'$group' : {'_id' : '$pub', 'count' : {'$sum' : 1}}},\n",
    "#     { '$sort' : {'count' : -1} }\n",
    "# ]\n",
    "# result = client['we1s'].command('aggregate', 'humanities_keywords', pipeline=pipeline, explain=False)\n",
    "# for row in result['cursor']['firstBatch']:\n",
    "#     print(row['count'], '\\t', row['_id'])\n",
    "\n",
    "# OUTPUT:\n",
    "# 89441 \t None\n",
    "# 55396 \t The New York Times\n",
    "# 30456 \t Los Angeles Times\n",
    "# 25541 \t Chicago Tribune\n",
    "# 23725 \t The Washington Post\n",
    "# 12741 \t The Irish Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ['thenewyorktimes'], 'count': 57700}\n",
      "{'_id': ['thewashingtonpost'], 'count': 37306}\n",
      "{'_id': ['thelatimes'], 'count': 34483}\n",
      "{'_id': ['chicagotribune'], 'count': 25541}\n",
      "{'_id': ['universitywire'], 'count': 24459}\n",
      "{'_id': ['theirishtimes'], 'count': 12741}\n",
      "{'_id': ['newsday'], 'count': 9656}\n",
      "{'_id': ['deseretmorningnewssaltlakecity'], 'count': 6496}\n",
      "{'_id': ['theguardianlondon'], 'count': 6414}\n",
      "{'_id': ['arkansasdemocratgazette'], 'count': 6231}\n",
      "{'_id': ['mirrorcouk'], 'count': 6136}\n",
      "{'_id': ['thehoustonchronicle'], 'count': 6071}\n",
      "{'_id': ['tveyesbbcradio4'], 'count': 5651}\n",
      "{'_id': ['tveyesbbcworld'], 'count': 5078}\n",
      "{'_id': ['targetednewsservice'], 'count': 5052}\n",
      "{'_id': ['globalbroadcastdatabase'], 'count': 4813}\n",
      "{'_id': ['bbcmonitoringinternationalreports'], 'count': 4538}\n",
      "{'_id': ['theindependentunitedkingdom'], 'count': 4291}\n",
      "{'_id': ['tveyesbbcradio5live'], 'count': 3401}\n",
      "{'_id': ['chronicleofhighereducation'], 'count': 3300}\n",
      "{'_id': ['thehartfordcourant'], 'count': 2836}\n",
      "{'_id': ['irishindependent'], 'count': 2769}\n",
      "{'_id': ['thedailytelegraph'], 'count': 2621}\n",
      "{'_id': ['thebostonglobe'], 'count': 2360}\n",
      "{'_id': None, 'count': 2333}\n",
      "{'_id': ['channelnewsasia'], 'count': 2191}\n",
      "{'_id': ['tveyesbbcradio2'], 'count': 2001}\n",
      "{'_id': ['tveyesbbc1northernireland'], 'count': 1925}\n",
      "{'_id': ['belfasttelegraph'], 'count': 1897}\n",
      "{'_id': ['tveyesbbc1wales'], 'count': 1873}\n",
      "{'_id': ['usatoday'], 'count': 1864}\n",
      "{'_id': ['argusleadersouthdakota'], 'count': 1786}\n",
      "{'_id': ['federalnewsservice'], 'count': 1719}\n",
      "{'_id': ['tveyesbbc1east'], 'count': 1702}\n",
      "{'_id': ['tveyesbbc1london'], 'count': 1699}\n",
      "{'_id': ['tveyesbbc1eastmidlands'], 'count': 1690}\n",
      "{'_id': ['theeveningstandardlondon'], 'count': 1680}\n",
      "{'_id': ['tveyesbbc1northwest'], 'count': 1677}\n",
      "{'_id': ['stlouispostdispatch'], 'count': 1652}\n",
      "{'_id': ['tveyesbbc1north'], 'count': 1649}\n",
      "{'_id': ['tveyesbbc2scotland'], 'count': 1616}\n",
      "{'_id': ['thebirminghamnews'], 'count': 1616}\n",
      "{'_id': ['tveyesbbc2'], 'count': 1609}\n",
      "{'_id': ['tveyesbbc1west'], 'count': 1588}\n",
      "{'_id': ['tveyesbbc1southwest'], 'count': 1577}\n",
      "{'_id': ['tveyesbbc1oxford'], 'count': 1573}\n",
      "{'_id': ['theoregonian'], 'count': 1552}\n",
      "{'_id': ['thedallasmorningnews'], 'count': 1551}\n",
      "{'_id': ['tveyesbbc1southeast'], 'count': 1547}\n",
      "{'_id': ['tveyesbbc1southampton'], 'count': 1528}\n",
      "{'_id': ['tveyesbbc1northeastandcumbria'], 'count': 1520}\n",
      "{'_id': ['tveyesbbc1yorkshireandlincolnshire'], 'count': 1510}\n",
      "{'_id': ['theadvocatebatonrouge'], 'count': 1499}\n",
      "{'_id': ['tveyesbbc1scotland'], 'count': 1493}\n",
      "{'_id': ['dailynews'], 'count': 1492}\n",
      "{'_id': ['scotsman'], 'count': 1320}\n",
      "{'_id': ['thedailyoklahomanoklahomacity'], 'count': 1318}\n",
      "{'_id': ['thewallstreetjournal'], 'count': 1299}\n",
      "{'_id': ['theknoxvillenewssentinel'], 'count': 1299}\n",
      "{'_id': ['theheraldglasgow'], 'count': 1291}\n",
      "{'_id': ['tveyesbbc2wales'], 'count': 1257}\n",
      "{'_id': ['thestarledgernewark'], 'count': 1248}\n",
      "{'_id': ['tveyesbbcradio1'], 'count': 1240}\n",
      "{'_id': ['elnuevoherald'], 'count': 1218}\n",
      "{'_id': ['tveyesbbcnews24'], 'count': 1180}\n",
      "{'_id': ['chicagodailyherald'], 'count': 1174}\n",
      "{'_id': ['easterndailypress'], 'count': 1135}\n",
      "{'_id': ['startribuneminneapolis'], 'count': 1124}\n",
      "{'_id': ['liverpoolecho'], 'count': 1081}\n",
      "{'_id': ['providencejournal'], 'count': 1080}\n",
      "{'_id': ['thecharlotteobserver'], 'count': 1057}\n",
      "{'_id': ['iindependentprintltd'], 'count': 1043}\n",
      "{'_id': ['bristolpost'], 'count': 1032}\n",
      "{'_id': ['irishnews'], 'count': 1027}\n",
      "{'_id': ['thealbuquerquejournal'], 'count': 1014}\n",
      "{'_id': ['dailystar'], 'count': 1006}\n",
      "{'_id': ['tveyesbbc4'], 'count': 991}\n",
      "{'_id': ['thedailymailandmailonsunday'], 'count': 975}\n",
      "{'_id': ['thephiladelphiainquirer'], 'count': 945}\n",
      "{'_id': ['thearizonarepublicphoenix'], 'count': 944}\n",
      "{'_id': ['theobserverlondon'], 'count': 907}\n",
      "{'_id': ['newyorkpost'], 'count': 906}\n",
      "{'_id': ['cnncom'], 'count': 897}\n",
      "{'_id': ['chicagodefender'], 'count': 891}\n",
      "{'_id': ['alaskadispatchnews'], 'count': 885}\n",
      "{'_id': ['newsweek'], 'count': 882}\n",
      "{'_id': ['thenorthernecho'], 'count': 853}\n",
      "{'_id': ['bismarcktribune'], 'count': 842}\n",
      "{'_id': ['lexingtonheraldleader'], 'count': 841}\n",
      "{'_id': ['theindianapolisstar'], 'count': 810}\n",
      "{'_id': ['thesaltlaketribune'], 'count': 799}\n",
      "{'_id': ['southwaleseveningpost'], 'count': 795}\n",
      "{'_id': ['cqcongressionaltestimony'], 'count': 778}\n",
      "{'_id': ['dailypostnorthwales'], 'count': 770}\n",
      "{'_id': ['omahaworldherald'], 'count': 738}\n",
      "{'_id': ['tveyesbbcparliament'], 'count': 712}\n",
      "{'_id': ['nottinghampost'], 'count': 712}\n",
      "{'_id': ['leicestermercury'], 'count': 702}\n",
      "{'_id': ['thedesmoinesregister'], 'count': 694}\n",
      "{'_id': ['thebaltimoresun'], 'count': 674}\n",
      "{'_id': ['daytondailynewsohio'], 'count': 671}\n",
      "CPU times: user 70.1 ms, sys: 29.4 ms, total: 99.4 ms\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# list source by article counts, descending\n",
    "# NOTE: distinct from pub (publications)\n",
    "pipeline = [\n",
    "    {'$group' : {'_id' : '$sources', 'count' : {'$sum' : 1}}},\n",
    "    { '$sort' : {'count' : -1} }\n",
    "]\n",
    "result = client['we1s'].command('aggregate', 'humanities_keywords', pipeline=pipeline, explain=False)\n",
    "for row in result['cursor']['firstBatch']:\n",
    "    print(row)\n",
    "\n",
    "## OUTPUT\n",
    "# {'_id': ['thenewyorktimes'], 'count': 57700}\n",
    "# {'_id': ['thewashingtonpost'], 'count': 37306}\n",
    "# {'_id': ['thelatimes'], 'count': 34483}\n",
    "# {'_id': ['chicagotribune'], 'count': 25541}\n",
    "# {'_id': ['universitywire'], 'count': 24459}\n",
    "# {'_id': ['theirishtimes'], 'count': 12741}\n",
    "# {'_id': ['newsday'], 'count': 9656}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find missing fields in one collection:\n",
      " EX: Find missing `title` from Sources.Sources\n",
      "\n",
      "\n",
      "found     empties   missing   key       \n",
      "1293      5         5         title     \n",
      "1298      counted   \n",
      "\n",
      "CPU times: user 10 ms, sys: 3.83 ms, total: 13.8 ms\n",
      "Wall time: 18.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# VERY SLOW ON LARGE COLLECTIONS\n",
    "\n",
    "print(\"Find missing fields in one collection:\\n \\\n",
    "EX: Find missing `title` from Sources.Sources\\n\")\n",
    "cursor = MongoClient('mongodb://mongo/')['Sources']['Sources'].find({}, {'title': 1})\n",
    "fcounter = FieldCounter(show_complete=True, includes=['title'])\n",
    "for doc in cursor:\n",
    "    fcounter.count_fields(doc)\n",
    "print(fcounter.report())\n",
    "\n",
    "## DEPRECATED: count a single field in a single collection\n",
    "# hits = 0\n",
    "# for doc in client['we1s']['humanities_keywords'].find({'pub_date': {'$exists': True, '$ne': []}}, {'pub_date':1}):\n",
    "#     hits += 1;\n",
    "# print(hits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count fields in each db.collection\n",
      "\n",
      "we1s2018.Sources(0 docs)\n",
      "[no docs]\n",
      "\n",
      "we1s2018.Corpus(548329 docs)\n",
      "\n",
      "found     empties   missing   key       \n",
      "249330    298998    298999    attachment_id\n",
      "548201    128       128       content   \n",
      "548328    0         1         doc_id    \n",
      "548097    231       232       length    \n",
      "452709    95276     95620     pub_date  \n",
      "344       0         547985    pub_short \n",
      "344       0         547985    search_term\n",
      "660       0         547669    site      \n",
      "1         0         548328    source_id \n",
      "660       0         547669    term      \n",
      "497141    51188     51188     title     \n",
      "1004      0         547325    url       \n",
      "548329    counted   \n",
      "\n",
      "we1s2018.testcollection(1 docs)\n",
      "[no fields empty/missing]\n",
      "\n",
      "CPU times: user 12.2 s, sys: 6.9 s, total: 19.1 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## VERY SLOW -- MANY MINUTES -- for e.g. we1s database, or whole server\n",
    "## TEST IS ~30 seconds.\n",
    "## THIS IS A KEY REPORT, BUT LOOPS COULD BE REFACTORED\n",
    "\n",
    "print('Count fields in each db.collection\\n')\n",
    "mfc = MongoCounter(MongoClient('mongodb://mongo/'))\n",
    "mfc.count_fields_db('we1s2018', progress=False, show_complete=False)\n",
    "\n",
    "# DISABLED BECAUSE VERY SLOW\n",
    "# print('Count fields in each db, each collection\\n')\n",
    "# mfc.count_fields_all(progress=False, show_complete=False) # everything, including deletes--extremely slow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports: Indexes\n",
    "\n",
    "List indexes present on all collections in a database, or across all databases on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List indexes on an individual collection\n",
      "\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.reddit'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.humanities_keywords_no_exact'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.comparison-not-humantiies-filter'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.humanities_keywords'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.comparison-sciences-filter'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.comparison-sciences'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.deletes_humanities'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.comparison-not-humanities'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.deletes_reddit'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.deletes_comparison-not-humanities'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.deletes_comparison-sciences'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s._config'}\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # fast\n",
    "\n",
    "client = MongoClient('mongodb://mongo/')\n",
    "print(\"\\nList indexes on an individual collection\\n\")\n",
    "report = report_indexes(db_colls(client, 'we1s'))\n",
    "for index in report:\n",
    "    print_SON(index)\n",
    "\n",
    "# # DEPRECATED\n",
    "# print(\"List indexes on an individual collection\")\n",
    "# idxs = client['we1s']['deletes_humanities'].list_indexes()\n",
    "# for idx in idxs:\n",
    "#     print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List indexes on all db collections\n",
      "\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'Sources.Sources'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'admin.system.version'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'app.apps'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'clientAppId': 1}, 'name': 'location_1_domain_id_hash_1_clientAppId_1', 'ns': 'app.apps'}\n",
      "{'v': 2, 'key': {'groupId': 1, 'product': 1}, 'name': 'groupId_1_product_1', 'ns': 'app.apps'}\n",
      "{'v': 2, 'key': {'eventSubscriptions._id': 1}, 'name': 'eventSubscriptions._id_1', 'ns': 'app.apps'}\n",
      "{'v': 2, 'key': {'eventSubscriptions.type': 1, 'eventSubscriptions.state.producerState': 1, 'eventSubscriptions.state.updatedAt': 1, 'eventSubscriptions.state.group_notified': 1}, 'name': 'eventSubscriptionsTypeProducerStateUpdatedAtGroupNotified', 'ns': 'app.apps'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'app.streams'}\n",
      "{'v': 2, 'key': {'servers.hostname': 1}, 'name': 'servers.hostname_1', 'ns': 'app.streams'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'app.metadata'}\n",
      "{'v': 2, 'unique': True, 'key': {'client_app_id': 1}, 'name': 'client_app_id_1', 'ns': 'app.metadata'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'app.deployments'}\n",
      "{'v': 2, 'key': {'app_id': 1, 'deployed_at': -1, '_id': -1}, 'name': 'app_id_1_deployed_at_-1__id_-1', 'ns': 'app.deployments'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'app.drafts'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'app._id': 1, 'user_id': 1}, 'name': 'location_1_domain_id_hash_1_app._id_1_user_id_1', 'ns': 'app.drafts'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'app.clientAppId': 1, 'user_id': 1}, 'name': 'location_1_domain_id_hash_1_app.clientAppId_1_user_id_1', 'ns': 'app.drafts'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'auth.passwords'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'domainId': 1, 'loginIds.id': 1}, 'name': 'globalSharding_domainId_loginId', 'ns': 'auth.passwords'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'auth.devices'}\n",
      "{'v': 2, 'key': {'domainId': 1, 'userId': 1, 'appId': 1, 'pushEntities.serviceId': 1}, 'name': 'domainId_1_userId_1_appId_1_pushEntities.serviceId_1', 'ns': 'auth.devices'}\n",
      "{'v': 2, 'key': {'appId': 1, 'pushEntities.serviceId': 1}, 'name': 'appId_1_pushEntities.serviceId_1', 'ns': 'auth.devices'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'auth.groups'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'domainId': 1, 'groupId': 1}, 'name': 'globalSharded_domainId_groupId', 'ns': 'auth.groups'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'auth.pushMessages'}\n",
      "{'v': 2, 'key': {'appId': 1, 'state': 1}, 'name': 'appId_1_state_1', 'ns': 'auth.pushMessages'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'auth.refreshTokens'}\n",
      "{'v': 2, 'key': {'domainId': 1, 'userId': 1}, 'name': 'domainId_1_userId_1', 'ns': 'auth.refreshTokens'}\n",
      "{'v': 2, 'key': {'expires': 1}, 'name': 'expires_1', 'ns': 'auth.refreshTokens', 'expireAfterSeconds': 1}\n",
      "{'v': 2, 'key': {'identity.id': 1, 'identity.providerId': 1}, 'name': 'identity.id_1_identity.providerId_1', 'ns': 'auth.refreshTokens'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'auth.apiKeys'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'hashedKey': 1}, 'name': 'globalSharded_hashedKey', 'ns': 'auth.apiKeys'}\n",
      "{'v': 2, 'key': {'userId': 1, 'domainId': 1, 'type': 1}, 'name': 'userId_1_domainId_1_type_1', 'ns': 'auth.apiKeys'}\n",
      "{'v': 2, 'key': {'domainId': 1, 'type': 1}, 'name': 'domainId_1_type_1', 'ns': 'auth.apiKeys'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'domainId': 1, 'name': 1}, 'name': 'globalSharded_domainId_name', 'ns': 'auth.apiKeys', 'partialFilterExpression': {'type': 'app'}}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'userId': 1, 'domainId': 1, 'name': 1}, 'name': 'globalSharded_userId_domainId_name', 'ns': 'auth.apiKeys', 'partialFilterExpression': {'type': 'user'}}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'auth.users'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'domainId': 1, 'userId': 1}, 'name': 'globalSharded_domainId_userId', 'ns': 'auth.users'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'domainId': 1, 'identities.id': 1, 'identities.providerId': 1}, 'name': 'globalSharded_domainId_identities.Ids', 'ns': 'auth.users', 'partialFilterExpression': {'identities.providerId': {'$exists': True}}}\n",
      "{'v': 2, 'key': {'location': 1, 'domain_id_hash': 1, 'domainId': 1, 'identities.providerType': 1, 'userId': 1}, 'name': 'globalSharded_domainId_identities.Types', 'ns': 'auth.users'}\n",
      "{'v': 2, 'key': {'creation_date': 1, 'identities.providerType': 1}, 'name': 'creation_date_identities.providerType', 'ns': 'auth.users'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'config.system.sessions'}\n",
      "{'v': 2, 'key': {'lastUse': 1}, 'name': 'lsidTTLIndex', 'ns': 'config.system.sessions', 'expireAfterSeconds': 1800}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'events.unordered_queue'}\n",
      "{'v': 2, 'key': {'state': 1, 'created_at': 1}, 'name': 'state_1_created_at_1', 'ns': 'events.unordered_queue'}\n",
      "{'v': 2, 'key': {'completed_at': 1}, 'name': 'completed_at_1', 'ns': 'events.unordered_queue', 'expireAfterSeconds': 120}\n",
      "{'v': 2, 'key': {'location': 1, 'resource_id': 1, 'created_at': 1}, 'name': 'location_1_resource_id_1_created_at_1', 'ns': 'events.unordered_queue'}\n",
      "{'v': 2, 'key': {'location': 1, 'state': 1, 'resource_id': 1}, 'name': 'location_1_state_1_resource_id_1', 'ns': 'events.unordered_queue'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'events.counters'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'app_id': 1, 'resource_id': 1}, 'name': 'location_1_domain_id_hash_1_app_id_1_resource_id_1', 'ns': 'events.counters'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'events.ordered_queue'}\n",
      "{'v': 2, 'key': {'state': 1}, 'name': 'state_1', 'ns': 'events.ordered_queue'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'resource_id': 1, 'seq_id': 1}, 'name': 'location_1_domain_id_hash_1_resource_id_1_seq_id_1', 'ns': 'events.ordered_queue'}\n",
      "{'v': 2, 'key': {'location': 1, 'resource_id': 1, 'seq_id': 1}, 'name': 'location_1_resource_id_1_seq_id_1', 'ns': 'events.ordered_queue'}\n",
      "{'v': 2, 'key': {'location': 1, 'state': 1, 'resource_id': 1, 'seq_id': 1, '_id': 1, 'app_id': 1}, 'name': 'location_1_state_1_resource_id_1_seq_id_1__id_1_app_id_1', 'ns': 'events.ordered_queue'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'hosting.assets'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'appId': 1, 'path': 1, 'user_id': 1}, 'name': 'globalSharding_appId_path_userId', 'ns': 'hosting.assets'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'hosting.usage'}\n",
      "{'v': 2, 'unique': True, 'key': {'location': 1, 'domain_id_hash': 1, 'appId': 1, 'timestamp': 1}, 'name': 'globalSharding_appId_timestamp', 'ns': 'hosting.usage'}\n",
      "{'v': 2, 'key': {'timestamp': 1}, 'name': 'timestamp_1', 'ns': 'hosting.usage', 'expireAfterSeconds': 5184000}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'local.startup_log'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'log.log'}\n",
      "{'v': 2, 'key': {'co_id': 1}, 'name': 'co_id_1', 'ns': 'log.log'}\n",
      "{'v': 2, 'key': {'appId': 1, 'domainId': 1, 'started': 1, '_id': 1}, 'name': 'appId_1_domainId_1_started_1__id_1', 'ns': 'log.log'}\n",
      "{'v': 2, 'key': {'started': 1, '_id': 1, 'api_type': 1}, 'name': 'started_1__id_1_api_type_1', 'ns': 'log.log'}\n",
      "{'v': 2, 'key': {'started': 1}, 'name': 'started_1', 'ns': 'log.log', 'expireAfterSeconds': 2592000}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'metadata.settings'}\n",
      "{'v': 2, 'key': {'tenantId': 1}, 'name': 'tenantId_1', 'ns': 'metadata.settings'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'metadata.dashboards'}\n",
      "{'v': 2, 'key': {'tenantId': 1}, 'name': 'tenantId_1', 'ns': 'metadata.dashboards'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'metadata.items'}\n",
      "{'v': 2, 'key': {'tenantId': 1}, 'name': 'tenantId_1', 'ns': 'metadata.items'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'metadata.datasources'}\n",
      "{'v': 2, 'key': {'tenantId': 1}, 'name': 'tenantId_1', 'ns': 'metadata.datasources'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'metadata.users'}\n",
      "{'v': 2, 'key': {'tenantId': 1}, 'name': 'tenantId_1', 'ns': 'metadata.users'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.reddit'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.humanities_keywords_no_exact'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.comparison-not-humantiies-filter'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.humanities_keywords'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.comparison-sciences-filter'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.comparison-sciences'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.deletes_humanities'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.comparison-not-humanities'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.deletes_reddit'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.deletes_comparison-not-humanities'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s.deletes_comparison-sciences'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s._config'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s2018.Sources'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s2018.Corpus'}\n",
      "{'v': 2, 'key': {'_id': 1}, 'name': '_id_', 'ns': 'we1s2018.testcollection'}\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # fast\n",
    "\n",
    "print(\"\\nList indexes on all db collections\\n\")\n",
    "\n",
    "client = MongoClient('mongodb://mongo/')\n",
    "report = report_indexes(client_colls(client))\n",
    "for index in report:\n",
    "    print_SON(index)\n",
    "\n",
    "# # DEPRECATED\n",
    "# mfc = MongoCounter(MongoClient('mongodb://mongo/'))\n",
    "# mfc.list_indexes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417733 \t None\n",
      "42 \t  humanities\n",
      "53 \t  liberal_arts\n",
      "474 \t humanities\n",
      "CPU times: user 63.5 ms, sys: 15.9 ms, total: 79.4 ms\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SLOW ~1-2 mins\n",
    "\n",
    "print(\"aggregate 'term' by counts, alphabetized\")\n",
    "client = MongoClient('mongodb://mongo/')\n",
    "pipeline = [\n",
    "    {'$group' : {'_id' : '$term', 'count' : {'$sum' : 1}}},\n",
    "    { '$sort' : {'_id' : 1} }\n",
    "]\n",
    "result = client['we1s'].command('aggregate', 'humanities_keywords', pipeline=pipeline, explain=False)\n",
    "for row in result['cursor']['firstBatch']:\n",
    "    print(row['count'], '\\t', row['_id'])\n",
    "\n",
    "## OUTPUT\n",
    "# 417733 \t None\n",
    "# 42 \t  humanities\n",
    "# 53 \t  liberal_arts\n",
    "# 474 \t humanities\n",
    "\n",
    "## note leading spaces affect alpha-sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregate 'pub' by counts, descending\n",
      "417733 \t None\n",
      "42 \t  humanities\n",
      "53 \t  liberal_arts\n",
      "474 \t humanities\n",
      "CPU times: user 66.3 ms, sys: 19.9 ms, total: 86.2 ms\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SLOW\n",
    "\n",
    "print(\"aggregate 'pub' by counts, descending\")\n",
    "client = MongoClient('mongodb://mongo/')\n",
    "pipeline_pub = [\n",
    "    {'$group' : {'_id' : '$pub', 'count' : {'$sum' : 1}}},\n",
    "    { '$sort' : {'count' : -1} }\n",
    "]\n",
    "result = client['we1s'].command('aggregate', 'humanities_keywords', pipeline=pipeline, explain=False)\n",
    "for row in result['cursor']['firstBatch']:\n",
    "    print(row['count'], '\\t', row['_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # SLOW\n",
    "\n",
    "# client = MongoClient('mongodb://mongo/')\n",
    "# pipeline_pub = [\n",
    "#     {'$group' : {'_id' : '$pub', 'count' : {'$sum' : 1}}},\n",
    "#     { '$sort' : {'count' : -1} }\n",
    "# ]\n",
    "# pipeline_term = [\n",
    "#     {'$group' : {'_id' : '$term', 'count' : {'$sum' : 1}}},\n",
    "#     { '$sort' : {'count' : -1} }\n",
    "# ]\n",
    "# pipeline = pipeline_term\n",
    "#\n",
    "## LOOP OVER EVERYTHING\n",
    "# d = dict((db, [collection for collection in client[db].list_collection_names()])\n",
    "#          for db in client.list_database_names())\n",
    "# for db in d:\n",
    "#     for coll in d[db]:\n",
    "#         result = client[db].command('aggregate', 'humanities_keywords', pipeline=pipeline, explain=False)\n",
    "#         for row in result:\n",
    "#             json.dumps(row, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE1S database setup\n",
    "# -- create indexes\n",
    "# result = client['we1s']['deletes_humanities'].create_index([('user_id', pymongo.ASCENDING)], unique=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all fields in all articles\n",
    "\n",
    "# d = dict((db, [collection for collection in client[db].list_collection_names()])\n",
    "#              for db in client.list_database_names())\n",
    "# for db in d:\n",
    "#     for coll in d[db]:\n",
    "#         print(db, coll)\n",
    "#         cursor = coll.find({})\n",
    "#         for doc in cursor:\n",
    "#             for key in doc:\n",
    "#                 c[key] += 1\n",
    "\n",
    "\n",
    "\n",
    "# fc = FieldCounter()\n",
    "# fc.config('', is_empty=True)\n",
    "\n",
    "# fc.default_rule(counter_name='empties', value=[None, ''])\n",
    "# fc.default_rule(counter_name='all')\n",
    "# fc.field_rule(counter_name='no_content', )\n",
    "\n",
    "\n",
    "\n",
    "# self.datestamp = datetime.today().strftime('%Y%m%d-%H:%M:%S')\n",
    "# datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "        \n",
    "# def my_FieldCounters_across_database:\n",
    "#     \"\"\"Iterate across all dbs, all collections, and compute a FieldCounter\n",
    "#     for each.\n",
    "#\n",
    "#     Deprecated. Replaced by a MongoFieldCounter class.\n",
    "#     \"\"\"\n",
    "#\n",
    "#     results = []\n",
    "#     d = dict((db, [collection for collection in client[db].list_collection_names()])\n",
    "#                  for db in client.list_database_names())\n",
    "#     for db in d:\n",
    "#         for coll in d[db]:\n",
    "#             fcounter = FieldCounter(name=db+'.'+coll)\n",
    "#             cursor = client[db][coll].find({})\n",
    "#             for doc in cursor:\n",
    "#                 if fcounter.total !=0 and fcounter.total % 5000 == 0:\n",
    "#                     fcounter.report()\n",
    "#                 fcounter.count_fields(doc)\n",
    "#             print('\\n\\n', '[FINAL]', db, coll)\n",
    "#             fcounter.report()\n",
    "#             results.append(fcounter)\n",
    "#\n",
    "#     for result in results:\n",
    "#         result.report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DEPRECATED BY FieldCounter\n",
    "\n",
    "# # list all fields in all articles\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# field_dict = Counter()\n",
    "# field_dict_empty = Counter()\n",
    "\n",
    "# def key_report(field_dict):\n",
    "#     print(\"\".join(str(word).ljust(10) for word in ('found', 'missing', 'key')))\n",
    "#     for key, value in sorted(field_dict.items()):\n",
    "#         if value != hits:\n",
    "#             print(\"\".join(str(word).ljust(10) for word in (value, hits-value, key)))\n",
    "#     print('---------- empty ----------')\n",
    "#     for key, value in field_dict_empty.items():\n",
    "#         print(value, '\\t\\t', key)\n",
    "\n",
    "# cursor = client['we1s']['humanities_keywords'].find({})\n",
    "# hits = 0\n",
    "# for doc in cursor:\n",
    "#     if hits !=0 and hits % 10000 == 0:\n",
    "#         print('\\n')\n",
    "#         print('----------', hits ,'----------')\n",
    "#         key_report(field_dict)\n",
    "# #     if hits > 5000:\n",
    "# #         break\n",
    "#     hits += 1\n",
    "#     for key, value in doc.items():\n",
    "#         if value:\n",
    "#             field_dict[key] += 1\n",
    "#         else:\n",
    "#             field_dict_empty[key] += 1\n",
    "# key_report(field_dict)\n",
    "\n",
    "# # for key, value in field_dict.items():\n",
    "# #     if value != hits:\n",
    "# #         print(\"\".join(str(word).ljust(10) for word in (value, hits-value, key)))\n",
    "\n",
    "# print('\\n\\n', field_dict)\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPRECATED -- OLD SOURCE IMPORT CODE\n",
    "\n",
    "#     def sources_doc_from_mongo(sources_template):\n",
    "#         result = client[db][collection].find_one({'_id' : sources_template['_id']})\n",
    "#         return result\n",
    "        \n",
    "#     def sources_doc_to_mongo(sources_template, source_name_aliases):\n",
    "#         sources_template['lookup_canonical'] = source_name_aliases\n",
    "        \n",
    "#         db, collection = sources_template['mongo_path']\n",
    "#         # result = client[db][collection].insert_one(sources_template)\n",
    "#         result = client[db][collection].replace_one({'_id':'source_name_aliases'}, sources_template)\n",
    "#         return result\n",
    "    \n",
    "        # ## SOURCES IMPORTER\n",
    "        # # imports Sources / Sources\n",
    "        # # ... and could import source_name_aliases to _config at the same \n",
    "        # source_name_aliases, source_docs = sources_csv_to_mongo('../sources_master.csv')\n",
    "        # # print('----------')\n",
    "        # # pp.pprint(source_docs)\n",
    "        # for key, source_doc in source_docs.items():\n",
    "        #     client['Sources']['Sources'].replace_one({'_id':source_doc['_id']}, source_doc, upsert=True)\n",
    "    \n",
    "        ## Old import _config source_name_aliases\n",
    "        # source_name_aliases = source_name_aliases_from_csv('../sources_canonical_title_test.csv') # outdated -- now _master.csv\n",
    "        # result = sources_doc_to_mongo(sources_template, source_name_aliases)\n",
    "        # print(result)\n",
    "        # result = sources_doc_from_mongo(sources_template)\n",
    "        # # print(result)\n",
    "\n",
    "#     def source_name_aliases_from_csv(filepath):\n",
    "#         \"\"\"build a dictionary of canonical source names and\n",
    "#         non-normalized title lookups from a csv file.\n",
    "    \n",
    "#         lookups are based on either 1) query slugs in filenames or\n",
    "#         2) pub fields from e.g. LexisNexis metadata--which are not\n",
    "#         normalized and may contain typographic erros. The source_name_aliases\n",
    "#         maps potential aliases against a canonical name, many to one.\n",
    "        \n",
    "#         The csv format is:\n",
    "#         title,name,canonical_title,tags,tags,tags,tags,tags,tags,country,language\n",
    "#         \"\"\"\n",
    "#         csvfile = open(filepath, 'r')\n",
    "#         # first row field names: title, canonical_name\n",
    "#         source_name_aliases = {}\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         # print(json.dumps( [ row for row in reader ] ))\n",
    "#         for row in reader:\n",
    "#             key = mdbkey_encode(row['title'])\n",
    "#             if key not in source_name_aliases:\n",
    "#                 source_name_aliases[key] = row['name'].strip()\n",
    "#             else:\n",
    "#                 print(\"duplicate key '{0}' found: '{1}'\".format(key, row['name'].strip()))\n",
    "#                 # raise ValueError(\"duplicate key '{0}' found\".format(key))\n",
    "#         # pp.pprint(source_name_aliases) ## LONG PREVIEW OUTPUT!\n",
    "#         return source_name_aliases\n",
    "\n",
    "\n",
    "# # client['Sources']['Sources'].replace_one({'_id':source_doc['_id']}, source_doc, upsert=True)\n",
    "# result = sources_doc_to_mongo(sources_template, source_name_aliases)\n",
    "# print(result)  \n",
    "\n",
    "# ## Old import _config source_name_aliases\n",
    "# source_name_aliases = source_name_aliases_from_csv('../sources_canonical_title_test.csv') # outdated -- now _master.csv\n",
    "# result = sources_doc_to_mongo(sources_template, source_name_aliases)\n",
    "# print(result)\n",
    "# result = sources_doc_from_mongo(sources_template)\n",
    "# # print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPRECATED: previous working version, added to ArticleProcessor class\n",
    "## displays side-by-side html views of json with pretty formatting.\n",
    "#\n",
    "# print('Rewrite in-memory, no mongo update')\n",
    "# coll = client['we1s']['deletes_humanities']\n",
    "# docs = coll.aggregate([{ '$sample': { 'size': 2 } }])\n",
    "# data =[]\n",
    "# pop_list = ['features','language_model','content-unscrubbed']\n",
    "# trim_dict={'content':500, 'attachment_id': 30, 'doc_id':40, 'metapath':40, 'content-hash-ssdeep': 30, 'name':40}\n",
    "# for doc in docs:\n",
    "#     before = doc_preview(doc, pop_list=pop_list, trim_dict=trim_dict, width=60)\n",
    "#     ap.json_update(doc)\n",
    "#     after = doc_preview(doc, pop_list=pop_list, trim_dict=trim_dict, width=60)\n",
    "#     data.append((before, after))\n",
    "# from IPython.display import display, HTML\n",
    "# # css_str = '<style>body{background-color:#000000}; table{width:600px !important}; td{width:200 !important};</style>'\n",
    "# css_str = '<style>td{border: 1px solid black} td{vertical-align: top}</style>'\n",
    "# display(HTML(\n",
    "# #     css_str + '<table style=\"text-align:left !important; background:red\"><tr>{}</tr></table>'.format(\n",
    "#     css_str + '<table><tr style>{}</tr></table>'.format(\n",
    "#         '</tr><tr>'.join(\n",
    "#         '<td><pre>{}</pre></td>'.format('</pre></td><td><pre>'.join(str(_) for _ in row)) for row in data)\n",
    "#     )), metadata=dict(isolated=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
