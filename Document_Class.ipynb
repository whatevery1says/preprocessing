{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the language model, data source, and options\n",
    "\n",
    "model = 'en_core_web_sm'\n",
    "manifest_dir = 'data'\n",
    "manifest_file = '2007_10_humanities_student_major_9_reddit_com.json'\n",
    "options = {'merge_noun_chunks': False, 'merge_subtokens': False, 'collect_readability_scores': True}\n",
    "add_stopwords = ['schmuck', 'ridiculous']\n",
    "remove_stopwords = ['of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from ftfy import fix_text\n",
    "from nltk.stem.porter import *  \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Constants for the preprocessing functions\n",
    "LINEBREAK_REGEX = re.compile(r'((\\r\\n)|[\\n\\v])+')\n",
    "NONBREAKING_SPACE_REGEX = re.compile(r'(?!\\n)\\s+')\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "# Add and remove custom stop words\n",
    "for word in add_stopwords:\n",
    "    nlp.vocab[word].is_stop = True\n",
    "for word in remove_stopwords:\n",
    "    nlp.vocab[word].is_stop = False\n",
    "\n",
    "# Test for the spacy-readability module\n",
    "try:\n",
    "    from spacy_readability import Readability\n",
    "except:\n",
    "    msg = 'The spacy-readability module is not installed on your system, so readability scores will be unavailable unless you `pip install spacy-_readability`.'\n",
    "    print(msg)\n",
    "\n",
    "# The Document class\n",
    "class Document():\n",
    "    \"\"\"Model a document's features.\n",
    "\n",
    "    Parameters:\n",
    "    - manifest_dir: the path to the manifest directory\n",
    "    - manifest_file: the name of the manifest file.\n",
    "    - content_property: the name of the property from which to extract the content\n",
    "\n",
    "    Returns a JSON object with the format `{'response': 'success|fail', 'errors': []}`.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, manifest_dir, manifest_file, content_property, **kwargs):\n",
    "        \"\"\"Initialize the object.\"\"\"\n",
    "        self.manifest_filepath = os.path.join(manifest_dir, manifest_file)\n",
    "        self.manifest_dict = self._read_manifest()\n",
    "        self.manifest_json = json.dumps(self.manifest_dict, indent=2)\n",
    "        self.doc_string = self.scrub(self._get_docstring(content_property))\n",
    "        self.content = nlp(self.doc_string)\n",
    "        self.options = kwargs['kwargs']\n",
    "        if 'features' in self.manifest_dict:\n",
    "            self.features = pd.DataFrame(\n",
    "                self.manifest_dict['features'],\n",
    "                columns=['Text', 'Lemma', 'POS', 'Tag', 'Entities']\n",
    "            )\n",
    "        else:\n",
    "            self.features = self.get_features()\n",
    "\n",
    "    def _remove_accents(self, text, method='unicode'):\n",
    "        \"\"\"Remove accents from any accented unicode characters in a string.\n",
    "\n",
    "        Either transforms them into ascii equivalents or removes them entirely.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): raw text\n",
    "        - method ({'unicode', 'ascii'}): if 'unicode', remove accented\n",
    "            char for any unicode symbol with a direct ASCII equivalent; if 'ascii',\n",
    "            remove accented char for any unicode symbol.\n",
    "            NB: the 'ascii' method is notably faster but less effective than 'unicode'.\n",
    "        Returns:\n",
    "            str\n",
    "        Raises:\n",
    "            ValueError: if ``method`` is not in {'unicode', 'ascii'}\n",
    "        \"\"\"\n",
    "        if method == 'unicode':\n",
    "            return ''.join(\n",
    "                c\n",
    "                for c in unicodedata.normalize('NFKD', text)\n",
    "                if not unicodedata.combining(c)\n",
    "            )\n",
    "        elif method == 'ascii':\n",
    "            return (\n",
    "                unicodedata.normalize('NFKD', text)\n",
    "                .encode('ascii', errors='ignore')\n",
    "                .decode('ascii')\n",
    "            )\n",
    "        else:\n",
    "            msg = '`method` must be either \"unicode\" and \"ascii\", not {}'.format(method)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "    def scrub(self, text, unicode_normalization='NFC', accent_removal_method='unicode'):\n",
    "        \"\"\"Normalize whitespace and and bad unicode, and remove accents.\n",
    "\n",
    "        Parameters:\n",
    "        - unicode_normalization: The ftfy.fix_text() `normalization` parameter.\n",
    "        - accent_removal_method: The Doc.remove_accents() `method` parameter.    \n",
    "        Returns str\n",
    "        \"\"\"\n",
    "        # Change multiple spaces to one and multiple line breaks to one.\n",
    "        # Also strip leading/trailing whitespace.\n",
    "        text = NONBREAKING_SPACE_REGEX.sub(' ', LINEBREAK_REGEX.sub(r'\\n', text)).strip()\n",
    "        # Combine characters and diacritics written using separate code points\n",
    "        text = fix_text(text, normalization=unicode_normalization)\n",
    "        text = self._remove_accents(text, method=accent_removal_method)\n",
    "        return text\n",
    "\n",
    "    def _read_manifest(self):\n",
    "        \"\"\"Read a JSON file and return a Python dict.\"\"\"\n",
    "        with open(self.manifest_filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.loads(f.read())\n",
    "        \n",
    "    def _get_docstring(self, content_property):\n",
    "        \"\"\"Extract a document string from a manifest property.\"\"\"\n",
    "        return self.manifest_dict[content_property]\n",
    "\n",
    "    def get_features(self):\n",
    "        \"\"\"Process the document with the spaCy pipeline into a pandas dataframe.\n",
    "        \n",
    "        If `collect_readability_scores` is set, Flesch-Kincaid Readability,\n",
    "        Flesch-Kincaid Reading Ease and Dale-Chall formula scores are collected\n",
    "        in a tuple in that order. Other formulas are available (see \n",
    "        https://github.com/mholtzscher/spacy_readability).\n",
    "        \n",
    "        Parameters:\n",
    "        - as_list: Return the features as a list instead of a dataframe\n",
    "        \"\"\"\n",
    "        # Handle optional pipes\n",
    "        if 'merge_noun_chunks' in self.options and self.options['merge_noun_chunks'] == True:\n",
    "            merge_nps = nlp.create_pipe('merge_noun_chunks')\n",
    "            nlp.add_pipe(merge_nps)\n",
    "        if 'merge_subtokens' in self.options and self.options['merge_subtokens'] == True:\n",
    "            merge_subtok = nlp.create_pipe('merge_subtokens')\n",
    "            nlp.add_pipe(merge_subtok)\n",
    "        if 'collect_readability_scores' in self.options and self.options['collect_readability_scores'] == True:\n",
    "            try:\n",
    "                nlp.add_pipe(Readability())\n",
    "            except:\n",
    "                pass\n",
    "            readability = (self.content._.flesch_kincaid_grade_level, self.content._.flesch_kincaid_reading_ease, self.content._.dale_chall)\n",
    "        # Build the feature list\n",
    "        feature_list = []\n",
    "        columns = ['TOKEN', 'LEMMA', 'POS', 'TAG', 'STOPWORD', 'ENTITIES']\n",
    "#         if readability:\n",
    "#             columns.append('READABILITY')\n",
    "        for token in self.content:\n",
    "            # Get named entity info (I=Inside, O=Outside, B=Begin)\n",
    "            ner = (token.ent_iob_, token.ent_type_)\n",
    "            t = [token.text, token.lemma_, token.pos_, token.tag_, str(token.is_stop), ner]\n",
    "#             if readability:\n",
    "#                 t.append(readability)\n",
    "            feature_list.append(tuple(t))\n",
    "        return pd.DataFrame(feature_list, columns=columns)\n",
    "\n",
    "    def filter(self, pattern=None, column='TOKEN', skip_punct=False, skip_stopwords=False, skip_linebreaks=False, case=True, flags=0, na=False, regex=True):\n",
    "        \"\"\"Return a new dataframe with filtered rows.\n",
    "\n",
    "        Parameters:\n",
    "        - pattern: The string or regex pattern on which to filter.\n",
    "        - column: The column where the string is to be searched.\n",
    "        - skip_punct: Do not include punctuation marks.\n",
    "        - skip_stopwords: Do not include stopwords.\n",
    "        - skip_linebreaks: Do not include linebreaks.\n",
    "        - case: Perform a case-sensitive match.\n",
    "        - flags: Regex flags.\n",
    "        - na: Filler for empty cells.\n",
    "        - regex: Set to True; otherwise absolute values will be matched.\n",
    "\n",
    "        The last four parameters are from `pandas.Series.str.contains`.\n",
    "        \"\"\"\n",
    "        # Filter based on column content\n",
    "        new_df = self.features\n",
    "        if pattern is not None:\n",
    "            new_df = new_df[new_df[column].str.contains(pattern, case=case, flags=flags, na=na, regex=regex)]\n",
    "        # Filter based on token type\n",
    "        if skip_punct == True:\n",
    "            new_df = new_df[~new_df['POS'].str.contains('PUNCT', case=True, flags=0, na=False, regex=True)]\n",
    "        if skip_stopwords == True:\n",
    "            new_df = new_df[~new_df['STOPWORD'].str.contains('TRUE', case=False, flags=0, na=False, regex=True)]\n",
    "        if skip_linebreaks == True:\n",
    "            new_df = new_df[~new_df['POS'].str.contains('SPACE', case=True, flags=0, na=False, regex=True)]\n",
    "        return new_df\n",
    "\n",
    "    def lemmas(self, as_list=False):\n",
    "        \"\"\"Return a dataframe containing just the lemmas.\"\"\"\n",
    "        if as_list == True:\n",
    "            return [token.lemma_ for token in self.content]\n",
    "        else:\n",
    "            return pd.DataFrame([token.lemma_ for token in self.content], columns=['LEMMA'])\n",
    "\n",
    "    def punctuation(self, as_list=False):\n",
    "        \"\"\"Return a dataframe containing just the punctuation marks.\"\"\"\n",
    "        if as_list == True:\n",
    "            return [token.text for token in self.content if token.is_punct]\n",
    "        else:\n",
    "            return pd.DataFrame([token.text for token in self.content if token.is_punct], columns=['PUNCTUATION'])\n",
    "\n",
    "    def pos(self, as_list=False):\n",
    "        \"\"\"Return a dataframe containing just the parts of speech.\"\"\"\n",
    "        if as_list == True:\n",
    "            return [token.pos_ for token in self.content]\n",
    "        else:\n",
    "            return pd.DataFrame([token.pos_ for token in self.content], columns=['POS'])\n",
    "\n",
    "    def tags(self, as_list=False):\n",
    "        \"\"\"Return a dataframe containing just the tags.\"\"\"\n",
    "        if as_list == True:\n",
    "            return [token.tag_ for token in self.content]\n",
    "        else:\n",
    "            return pd.DataFrame([token.tag_ for token in self.content], columns=['TAG'])\n",
    "\n",
    "    def entities(self, options=['text', 'label'], as_list=False):\n",
    "        \"\"\"Return a dataframe containing just the entities from the document.\n",
    "        \n",
    "        Parameters:\n",
    "        - options: a list of attributes ('text', 'start', 'end', 'label')\n",
    "        - as_list: return the entities as a list of tuples.\n",
    "        \"\"\"\n",
    "        ents = []\n",
    "        for ent in self.content.ents:\n",
    "            e = []\n",
    "            if 'text' in options:\n",
    "                e.append(ent.text)\n",
    "            if 'start' in options:\n",
    "                e.append(ent.start)\n",
    "            if 'end' in options:\n",
    "                e.append(ent.end)\n",
    "            if 'label' in options:\n",
    "                e.append(ent.label_)\n",
    "            ents.append(tuple(e))\n",
    "        if as_list == True:\n",
    "            return ents\n",
    "        else:\n",
    "            return pd.DataFrame(ents, columns=[option.title() for option in options])\n",
    "\n",
    "    def readability_scores(self, columns=['Flesch-Kincaid Readability',\n",
    "        'Flesch-Kincaid Reading Ease', 'Dale-Chall'], as_list=False):\n",
    "        \"\"\"Get a list of readability scores from the document.\n",
    "        \n",
    "        Parameters:\n",
    "        - columns: a list of labels for the score types\n",
    "        - as_df: return the list as a dataframe.\n",
    "        \"\"\"\n",
    "        fkr = self.content._.flesch_kincaid_reading_ease\n",
    "        fkg = self.content._.flesch_kincaid_grade_level\n",
    "        dc = self.content._.dale_chall\n",
    "        scores = [(fkr, fkg, dc)]\n",
    "        if as_list == True:\n",
    "            return scores\n",
    "        else:\n",
    "            return pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    def stems(self, stemmer='porter', as_list=False):\n",
    "        \"\"\"Convert the tokens in a spaCy document to stems.\n",
    "\n",
    "        Parameters:\n",
    "        - stemmer: the stemming algorithm ('porter' or 'snowball').\n",
    "        - as_list: return the dataframe as a list.\n",
    "        \"\"\"\n",
    "        if stemmer == 'snowball':\n",
    "            stemmer = SnowballStemmer(language='english')\n",
    "        else:\n",
    "            stemmer = PorterStemmer()\n",
    "        stems = [stemmer.stem(token.text) for token in self.content]\n",
    "        if as_list == True:\n",
    "            return stems\n",
    "        else:\n",
    "            return pd.DataFrame(stems, columns=['Stems'])\n",
    "\n",
    "    def ngrams(self, n=2, as_list=False):\n",
    "        \"\"\"Convert the tokens in a spaCy document to ngrams.\n",
    "\n",
    "        Parameters:\n",
    "        - n: The number of tokens in an ngram.\n",
    "        - as_list: return the dataframe as a list.\n",
    "        \"\"\"\n",
    "        ngram_tokens = list(ngrams([token.text for token in self.content], n))\n",
    "        if as_list == True:\n",
    "            return ngram_tokens\n",
    "        else:\n",
    "            prefix = str(n) + '-'\n",
    "            if n == 2:\n",
    "                prefix = 'Bi'\n",
    "            if n == 3:\n",
    "                prefix = 'Tri'\n",
    "            label = prefix + 'grams'\n",
    "            return pd.DataFrame({label: pd.Series(ngram_tokens)})\n",
    "\n",
    "def bagify(df, skip_punct=False, skip_stopwords=False, pos=None, as_counter=False):\n",
    "    \"\"\"Convert a spaCy document into a dict containing feature frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    - remove: a list containing features to be removed ('punctuation', 'stopwords', or both)\n",
    "    - pos: a valid grammatical category to filter by (e.g. 'NOUN').\n",
    "    - as_counter: returns the result as a counter object\n",
    "\n",
    "    Returns a dict unless as_counter is set to `True`. This enables the following:\n",
    "\n",
    "    `bag = features.bagify(as_counter=True).most_common(10)`\n",
    "    \"\"\"\n",
    "    print('moo')\n",
    "#     # Boolean False is not accepted for some reason\n",
    "#     if pos is not None:\n",
    "#         bag = [token.text for token in content if token.is_punct != skip_punct and token.is_stop != skip_stop and token.pos_ == pos]\n",
    "#     else:\n",
    "#         bag = [token.text for token in self.content if token.is_punct != skip_punct and token.is_stop != skip_stop]\n",
    "#     if as_counter == True:\n",
    "#         return Counter(bag)\n",
    "#     else:\n",
    "#         return dict(Counter(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the feature table\n",
    "doc = Document(manifest_dir, manifest_file, 'content_scrubbed', kwargs=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Don\\'t listen to any of these schmucks about how you should/should not have ridiculous amounts of sex. That[.] your personal decision and yours alone.\\n2. Visit the schools you apply to. Only so much can be conveyed about a school through a pamphlet/website/phone recruiter - and you\\'d better be sure as hell that perspective is a meticulously crafted one. Go and see it for yourself before you commit.\\n3. Don\\'t worry too much about going to a competitive school with a world-renowned reputation... even if you are worried about getting into other schools after you graduate. I went to a very small liberal_arts college for my undergrad and now I am a PhD candidate at an Ivy League university in a highly competitive biology program. Truth be told, your grades that you earn during your four years, as well as the recommendations you get from people for your graduate school applications are what truly count. 4. Don\\'t get scared about picking the \"right\" major, and don\\'t let anyone muscle you into something you don\\'t like. If you need the time, take the time to declare. You have one to two years before most schools will even let you. But... do devote a lot of time think about it. Don\\'t put it off. Oh, also, follow a career pathway that you enjoy, not something that is/will be \"hot\" or \"profitable\" or \"lucrative.\" It may or may not stay that way, and I\\'ll tell you firsthand that if you do not have a passion for what you do, it will make your life more challenging than it should be and you most likely will not have the stamina and happiness that make you feel fulfilled at the end of the day.\\n5. Remember... your life will be built in baby steps and don\\'t feel like you have to plan everything perfectly right now. Just be willing to accept change and to work for positive things in your life and you will do just fine.\\n6. And don\\'t forget to take time for yourself! College demands a lot of self-discipline in order to go to classes and complete your assignments, etc. and it is very easy to get so caught up in it that you forget to take sanity time. And it will catch up with you. So chill with friends, go hiking, or play extreme frisbee... whatever makes you happy and puts you back in touch with your core being.\\nBest of luck to you!'"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Uncomment lines below to try various class methods.\n",
    "## See the method docstrings for options not shown in the examples below.\n",
    "doc.doc_string\n",
    "# Get all the doc features in a dataframe\n",
    "# doc.get_features()\n",
    "\n",
    "# Count the number of feature unfiltered tokens\n",
    "# len(doc.get_features())\n",
    "\n",
    "# Filter the tokens\n",
    "# filtered = doc.filter(pattern='NOUN', column='POS', skip_punct=True, skip_stopwords=True, skip_linebreaks=True)\n",
    "# pd.DataFrame(filtered['TOKEN'])\n",
    "# len(filtered)\n",
    "\n",
    "# Get Various Features\n",
    "# doc.lemmas()\n",
    "# doc.punctuation(as_list=True)\n",
    "# doc.pos()\n",
    "# doc.tags()\n",
    "# doc.entities()\n",
    "# doc.readability_scores()\n",
    "# doc.stems(stemmer='porter')\n",
    "# doc.ngrams(n=2)\n",
    "\n",
    "# Insert a column - does not currently update the Doc.features\n",
    "# stems = doc.stems()\n",
    "# features = doc.get_features()\n",
    "# features.insert(3, 'STEM', stems)\n",
    "# features[0:10]\n",
    "\n",
    "# Bagify\n",
    "# doc.bagify()\n",
    "\n",
    "# Get most common nouns\n",
    "# doc.bagify(pos='NOUN', as_counter=True).most_common(5)\n",
    "\n",
    "# Get a count of all tokens\n",
    "# doc.token_count(remove=['punctuation', 'stopwords'])\n",
    "\n",
    "# Get the original document text\n",
    "# doc.doc_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "\n",
    "- Add custom tokenisation rules.\n",
    "- `Doc.bagify()` needs work.\n",
    "- Method to add a column (e.g. stems) to the feature list.\n",
    "- Methods to update the document manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
